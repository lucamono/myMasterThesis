\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{i}{chapter*.1}}
\newlabel{ch:symbols}{{}{ii}{Nomenclature}{chapter*.2}{}}
\newlabel{ch:symbols@cref}{{}{ii}}
\@writefile{toc}{\contentsline {chapter}{Nomenclature}{ii}{chapter*.2}}
\newlabel{sec:acronyms}{{}{ii}{Acronyms and Abbreviations}{section*.3}{}}
\newlabel{sec:acronyms@cref}{{}{ii}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:intro}{{1}{1}{Introduction}{chapter.1}{}}
\newlabel{ch:intro@cref}{{[chapter][1][]1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem Statement and Motivations }{1}{section.1.1}}
\newlabel{sec:motivations}{{1.1}{1}{Problem Statement and Motivations}{section.1.1}{}}
\newlabel{sec:motivations@cref}{{[section][1][1]1.1}{1}}
\citation{research_grasp}
\citation{hybrid_approach}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf  {Vision and Grasping at work example.} The robot detects objects before picking them up.\relax }}{2}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:hand-eye-robot-example}{{1.1}{2}{\textbf {Vision and Grasping at work example.} The robot detects objects before picking them up.\relax }{figure.caption.7}{}}
\newlabel{fig:hand-eye-robot-example@cref}{{[figure][1][1]1.1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Related Works }{2}{section.1.2}}
\newlabel{sec:related_works}{{1.2}{2}{Related Works}{section.1.2}{}}
\newlabel{sec:related_works@cref}{{[section][2][1]1.2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Object Grasping}{2}{subsection.1.2.1}}
\citation{rectangle_7D}
\citation{epsilon_grasp}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf  {The ARMAR-III grasping.} ARMAR-III is grasping a mashed potatoes box, captured by the robot’s camera system.\relax }}{3}{figure.caption.8}}
\newlabel{fig:armar}{{1.2}{3}{\textbf {The ARMAR-III grasping.} ARMAR-III is grasping a mashed potatoes box, captured by the robot’s camera system.\relax }{figure.caption.8}{}}
\newlabel{fig:armar@cref}{{[figure][2][1]1.2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces \textbf  {The grasping rectangle.} On left: the rgb image; on right: the registered depth map. The oriented rectangle indicates where to grasp the shoe and the gripper’s orientation. The gripper’s opening width and its physical size are represented by the red and blue lines.\relax }}{3}{figure.caption.9}}
\newlabel{fig:shoes_grasp}{{1.3}{3}{\textbf {The grasping rectangle.} On left: the rgb image; on right: the registered depth map. The oriented rectangle indicates where to grasp the shoe and the gripper’s orientation. The gripper’s opening width and its physical size are represented by the red and blue lines.\relax }{figure.caption.9}{}}
\newlabel{fig:shoes_grasp@cref}{{[figure][3][1]1.3}{3}}
\citation{motion}
\citation{cornell}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces \textbf  {Effect of pose uncertainty on the force closure of a grasp example.} On left: planned grasp from a database of preplanned grasps with an $\epsilon _{GWS}$ of 0.17; on right: the same grasp after a 20 degrees clockwise rotation and 1 cm translation. This perturbation results in a non-force closed grasp.\relax }}{4}{figure.caption.10}}
\newlabel{fig:epsilon_grasped}{{1.4}{4}{\textbf {Effect of pose uncertainty on the force closure of a grasp example.} On left: planned grasp from a database of preplanned grasps with an $\epsilon _{GWS}$ of 0.17; on right: the same grasp after a 20 degrees clockwise rotation and 1 cm translation. This perturbation results in a non-force closed grasp.\relax }{figure.caption.10}{}}
\newlabel{fig:epsilon_grasped@cref}{{[figure][4][1]1.4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Learning and Grasping}{4}{subsection.1.2.2}}
\citation{deep_grasp}
\citation{deep_full_image}
\citation{real_deep1}
\citation{real_deep2}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces \textbf  {The Cornell Dataset.} On Cornell Grasping Dataset, each object has multiple labelled grasps.\relax }}{5}{figure.caption.11}}
\newlabel{fig:cornell}{{1.5}{5}{\textbf {The Cornell Dataset.} On Cornell Grasping Dataset, each object has multiple labelled grasps.\relax }{figure.caption.11}{}}
\newlabel{fig:cornell@cref}{{[figure][5][1]1.5}{5}}
\citation{graspit}
\citation{dart}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces \textbf  {Parallel robots attempting grasp.} In this example, the acquisition of the dataset is performed for thousand hours on parallel robots until a very large number of acquisition is done.\relax }}{6}{figure.caption.12}}
\newlabel{fig:parallel_robots}{{1.6}{6}{\textbf {Parallel robots attempting grasp.} In this example, the acquisition of the dataset is performed for thousand hours on parallel robots until a very large number of acquisition is done.\relax }{figure.caption.12}{}}
\newlabel{fig:parallel_robots@cref}{{[figure][6][1]1.6}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces \textbf  {GraspIt! simulator.} 3D user interface allowing the user to interact with a virtual world containing robots, objects and obstacles. Each grasp is evaluated with numeric quality measures.\relax }}{6}{figure.caption.13}}
\newlabel{fig:graspit}{{1.7}{6}{\textbf {GraspIt! simulator.} 3D user interface allowing the user to interact with a virtual world containing robots, objects and obstacles. Each grasp is evaluated with numeric quality measures.\relax }{figure.caption.13}{}}
\newlabel{fig:graspit@cref}{{[figure][7][1]1.7}{6}}
\citation{deepLearningPose}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces \textbf  {DART simulator.} DART simulator includes physical forces and dynamic modelling during the grasping.\relax }}{7}{figure.caption.14}}
\newlabel{fig:dart}{{1.8}{7}{\textbf {DART simulator.} DART simulator includes physical forces and dynamic modelling during the grasping.\relax }{figure.caption.14}{}}
\newlabel{fig:dart@cref}{{[figure][8][1]1.8}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contributions}{7}{section.1.3}}
\newlabel{sec:relatedwork}{{1.3}{7}{Contributions}{section.1.3}{}}
\newlabel{sec:relatedwork@cref}{{[section][3][1]1.3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces \textbf  {Spqr@Work team.} Me with my team during the Robocup2018 GermanOpen competition in Magdeburg.\relax }}{8}{figure.caption.15}}
\newlabel{fig:robocup}{{1.9}{8}{\textbf {Spqr@Work team.} Me with my team during the Robocup2018 GermanOpen competition in Magdeburg.\relax }{figure.caption.15}{}}
\newlabel{fig:robocup@cref}{{[figure][9][1]1.9}{8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Robot Grasping with Anthropomorphic Robots}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:robotgrasping}{{2}{9}{Robot Grasping with Anthropomorphic Robots}{chapter.2}{}}
\newlabel{ch:robotgrasping@cref}{{[chapter][2][]2}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Applications}{9}{section.2.1}}
\newlabel{sec:industrial_robotics}{{2.1}{9}{Applications}{section.2.1}{}}
\newlabel{sec:industrial_robotics@cref}{{[section][1][2]2.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Pick and Place and Kitting Applications}{9}{subsection.2.1.1}}
\newlabel{subsec:pick_and_place_kitting_tasks}{{2.1.1}{9}{Pick and Place and Kitting Applications}{subsection.2.1.1}{}}
\newlabel{subsec:pick_and_place_kitting_tasks@cref}{{[subsection][1][2,1]2.1.1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \textbf  {Pick and place example.} The robot performs objects' grasping to release they in specific containers.\relax }}{10}{figure.caption.16}}
\newlabel{fig:pick_and_place}{{2.1}{10}{\textbf {Pick and place example.} The robot performs objects' grasping to release they in specific containers.\relax }{figure.caption.16}{}}
\newlabel{fig:pick_and_place@cref}{{[figure][1][2]2.1}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \textbf  {Youbot Pick and place example.} The SPQR Youbot during a Robocup competition. In this task, it attempts to release the bearing to the workstation.\relax }}{11}{figure.caption.17}}
\newlabel{fig:youbot_pp}{{2.2}{11}{\textbf {Youbot Pick and place example.} The SPQR Youbot during a Robocup competition. In this task, it attempts to release the bearing to the workstation.\relax }{figure.caption.17}{}}
\newlabel{fig:youbot_pp@cref}{{[figure][2][2]2.2}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \textbf  {Kitting task example.} A robot assembles Swiss knives in 3 containers.\relax }}{12}{figure.caption.18}}
\newlabel{fig:pick_and_place}{{2.3}{12}{\textbf {Kitting task example.} A robot assembles Swiss knives in 3 containers.\relax }{figure.caption.18}{}}
\newlabel{fig:pick_and_place@cref}{{[figure][3][2]2.3}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \textbf  {Path Recording example.} Picture shows an operator dealing with the correct positioning of the robot joints.\relax }}{12}{figure.caption.19}}
\newlabel{fig:pick_and_place}{{2.4}{12}{\textbf {Path Recording example.} Picture shows an operator dealing with the correct positioning of the robot joints.\relax }{figure.caption.19}{}}
\newlabel{fig:pick_and_place@cref}{{[figure][4][2]2.4}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}The Random Bin-Picking task}{12}{subsection.2.1.2}}
\newlabel{subsec:random_bin_task}{{2.1.2}{12}{The Random Bin-Picking task}{subsection.2.1.2}{}}
\newlabel{subsec:random_bin_task@cref}{{[subsection][2][2,1]2.1.2}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces \textbf  {Random Bin-Picking example.} In this picture, a heat treat machine tending cell uses a 3D vision guided robot equipped with a dual-head end effector to locate and pick randomly stacked automotive parts from a large bin.\relax }}{13}{figure.caption.20}}
\newlabel{fig:rbp}{{2.5}{13}{\textbf {Random Bin-Picking example.} In this picture, a heat treat machine tending cell uses a 3D vision guided robot equipped with a dual-head end effector to locate and pick randomly stacked automotive parts from a large bin.\relax }{figure.caption.20}{}}
\newlabel{fig:rbp@cref}{{[figure][5][2]2.5}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \textbf  {Execution of RBP localization algorithm with a 3D sensor vision.} In this picture, 3D area sensor maps the positions of multiple parts in a bin.\relax }}{14}{figure.caption.21}}
\newlabel{fig:rbp_alg}{{2.6}{14}{\textbf {Execution of RBP localization algorithm with a 3D sensor vision.} In this picture, 3D area sensor maps the positions of multiple parts in a bin.\relax }{figure.caption.21}{}}
\newlabel{fig:rbp_alg@cref}{{[figure][6][2]2.6}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Tools}{14}{section.2.2}}
\newlabel{sec:tools}{{2.2}{14}{Tools}{section.2.2}{}}
\newlabel{sec:tools@cref}{{[section][2][2]2.2}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Grippers}{14}{subsection.2.2.1}}
\newlabel{subsec:parallel_grip}{{2.2.1}{14}{Grippers}{subsection.2.2.1}{}}
\newlabel{subsec:parallel_grip@cref}{{[subsection][1][2,2]2.2.1}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces \textbf  {A monkey grabbing a stick.} The opposable thumb differentiates man from the rest of the animal world, making it in fact the most evolved species in the world able to grasp and manipulate objects.\relax }}{15}{figure.caption.22}}
\newlabel{fig:monkey}{{2.7}{15}{\textbf {A monkey grabbing a stick.} The opposable thumb differentiates man from the rest of the animal world, making it in fact the most evolved species in the world able to grasp and manipulate objects.\relax }{figure.caption.22}{}}
\newlabel{fig:monkey@cref}{{[figure][7][2]2.7}{15}}
\@writefile{toc}{\contentsline {subsubsection}{2.2.1.1$\hskip 1em\relax $Parallel Grippers}{15}{section*.23}}
\newlabel{subsubsec:parallel_grip}{{2.2.1}{15}{2.2.1.1$\quad $Parallel Grippers}{section*.23}{}}
\newlabel{subsubsec:parallel_grip@cref}{{[subsection][1][2,2]2.2.1}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces \textbf  {Schunk WSG.} A servo-electric 2 finger parallel gripper with sensitive gripping force control and long stroke.\relax }}{16}{figure.caption.24}}
\newlabel{fig:par_grip}{{2.8}{16}{\textbf {Schunk WSG.} A servo-electric 2 finger parallel gripper with sensitive gripping force control and long stroke.\relax }{figure.caption.24}{}}
\newlabel{fig:par_grip@cref}{{[figure][8][2]2.8}{16}}
\@writefile{toc}{\contentsline {subsubsection}{2.2.1.2$\hskip 1em\relax $Parallel-Jaw Grippers}{16}{section*.25}}
\newlabel{subsubsection:parallel_jaw_grip}{{2.2.1}{16}{2.2.1.2$\quad $Parallel-Jaw Grippers}{section*.25}{}}
\newlabel{subsubsection:parallel_jaw_grip@cref}{{[subsection][1][2,2]2.2.1}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces \textbf  {SPQR@Work team Robocup gripper.} A custom parallel-jaw gripper mounted in RoCoCo Lab, Sapienza.\relax }}{16}{figure.caption.26}}
\newlabel{fig:par_jaw_grip}{{2.9}{16}{\textbf {SPQR@Work team Robocup gripper.} A custom parallel-jaw gripper mounted in RoCoCo Lab, Sapienza.\relax }{figure.caption.26}{}}
\newlabel{fig:par_jaw_grip@cref}{{[figure][9][2]2.9}{16}}
\@writefile{toc}{\contentsline {subsubsection}{2.2.1.3$\hskip 1em\relax $Three Finger Grippers}{17}{section*.27}}
\newlabel{subsubsection:three_finger_grip}{{2.2.1}{17}{2.2.1.3$\quad $Three Finger Grippers}{section*.27}{}}
\newlabel{subsubsection:three_finger_grip@cref}{{[subsection][1][2,2]2.2.1}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \textbf  {Robotiq 3FAG.} The 3-Finger Adaptive Gripper is ideal for advanced manufacturing and robotic research. It adapts to the object’s shape for a solid grip.\relax }}{17}{figure.caption.28}}
\newlabel{fig:3_fing_grip}{{2.10}{17}{\textbf {Robotiq 3FAG.} The 3-Finger Adaptive Gripper is ideal for advanced manufacturing and robotic research. It adapts to the object’s shape for a solid grip.\relax }{figure.caption.28}{}}
\newlabel{fig:3_fing_grip@cref}{{[figure][10][2]2.10}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces \textbf  {3 Finger Gripper operational mode.} The basic mode is the most versatile Operation Mode. It is best suited for objects that have one dimension longer than the other two. It can grip a large variety of objects. The wide mode is optimal for gripping round or large objects. The pinch mode is used for small objects that have to be picked precisely. This Operation Mode can only grip objects between the distal phalanxes of the fingers. The scissor mode is used primarily for tiny objects. This mode is less powerful than the other three modes, but is precise. In scissor mode, it is not possible to surround an object.\relax }}{18}{figure.caption.29}}
\newlabel{fig:3_fing_grip}{{2.11}{18}{\textbf {3 Finger Gripper operational mode.} The basic mode is the most versatile Operation Mode. It is best suited for objects that have one dimension longer than the other two. It can grip a large variety of objects. The wide mode is optimal for gripping round or large objects. The pinch mode is used for small objects that have to be picked precisely. This Operation Mode can only grip objects between the distal phalanxes of the fingers. The scissor mode is used primarily for tiny objects. This mode is less powerful than the other three modes, but is precise. In scissor mode, it is not possible to surround an object.\relax }{figure.caption.29}{}}
\newlabel{fig:3_fing_grip@cref}{{[figure][11][2]2.11}{18}}
\@writefile{toc}{\contentsline {subsubsection}{2.2.1.4$\hskip 1em\relax $Vacuum Grippers}{18}{section*.30}}
\newlabel{subsubsection:parallel_jaw}{{2.2.1}{18}{2.2.1.4$\quad $Vacuum Grippers}{section*.30}{}}
\newlabel{subsubsection:parallel_jaw@cref}{{[subsection][1][2,2]2.2.1}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces \textbf  {Vacuum Gripper.} Vacuum gripping systems are used in a wide variety of industries to ensure efficient material flows. Pictures shows the Vacuum gripper used for this work of thesis.\relax }}{19}{figure.caption.31}}
\newlabel{fig:vacuum_grip}{{2.12}{19}{\textbf {Vacuum Gripper.} Vacuum gripping systems are used in a wide variety of industries to ensure efficient material flows. Pictures shows the Vacuum gripper used for this work of thesis.\relax }{figure.caption.31}{}}
\newlabel{fig:vacuum_grip@cref}{{[figure][12][2]2.12}{19}}
\@writefile{toc}{\contentsline {subsubsection}{2.2.1.5$\hskip 1em\relax $Micro and Nano Grippers}{19}{section*.32}}
\newlabel{subsubsection:micro_and_nano}{{2.2.1}{19}{2.2.1.5$\quad $Micro and Nano Grippers}{section*.32}{}}
\newlabel{subsubsection:micro_and_nano@cref}{{[subsection][1][2,2]2.2.1}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces \textbf  {Micro Gripper.} A micro gripper used for grasping electronic components.\relax }}{19}{figure.caption.33}}
\newlabel{fig:micro_grip}{{2.13}{19}{\textbf {Micro Gripper.} A micro gripper used for grasping electronic components.\relax }{figure.caption.33}{}}
\newlabel{fig:micro_grip@cref}{{[figure][13][2]2.13}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Sensors}{20}{subsection.2.2.2}}
\newlabel{subsection:sensors}{{2.2.2}{20}{Sensors}{subsection.2.2.2}{}}
\newlabel{subsection:sensors@cref}{{[subsection][2][2,2]2.2.2}{20}}
\@writefile{toc}{\contentsline {subsubsection}{2.2.2.1$\hskip 1em\relax $RGB cameras}{20}{section*.34}}
\newlabel{subsubsection:rgb_cameras}{{2.2.2}{20}{2.2.2.1$\quad $RGB cameras}{section*.34}{}}
\newlabel{subsubsection:rgb_cameras@cref}{{[subsection][2][2,2]2.2.2}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces \textbf  {Pinhole camera geometry.} $C$ is the camera centre and $p$ the principal point. $C$ is placed at the coordinate origin; $p$ is placed in front of the camera centre.\relax }}{20}{figure.caption.35}}
\newlabel{fig:camera_img}{{2.14}{20}{\textbf {Pinhole camera geometry.} $C$ is the camera centre and $p$ the principal point. $C$ is placed at the coordinate origin; $p$ is placed in front of the camera centre.\relax }{figure.caption.35}{}}
\newlabel{fig:camera_img@cref}{{[figure][14][2]2.14}{20}}
\newlabel{eq:mapping_points}{{2.1}{20}{2.2.2.1$\quad $RGB cameras}{equation.2.2.1}{}}
\newlabel{eq:mapping_points@cref}{{[equation][1][2]2.1}{20}}
\newlabel{eq:central_proj_homog}{{2.2}{21}{2.2.2.1$\quad $RGB cameras}{equation.2.2.2}{}}
\newlabel{eq:central_proj_homog@cref}{{[equation][2][2]2.2}{21}}
\newlabel{eq:compact_central_proj_homg}{{2.3}{21}{2.2.2.1$\quad $RGB cameras}{equation.2.2.3}{}}
\newlabel{eq:compact_central_proj_homg@cref}{{[equation][3][2]2.3}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces \textbf  {Image frame example.} Picture shows the Image $(x,y)$ and camera $(x_{cam},y_{cam})$ coordinate systems.\relax }}{21}{figure.caption.36}}
\newlabel{fig:principal_points}{{2.15}{21}{\textbf {Image frame example.} Picture shows the Image $(x,y)$ and camera $(x_{cam},y_{cam})$ coordinate systems.\relax }{figure.caption.36}{}}
\newlabel{fig:principal_points@cref}{{[figure][15][2]2.15}{21}}
\newlabel{eq:princ_points}{{2.4}{21}{2.2.2.1$\quad $RGB cameras}{equation.2.2.4}{}}
\newlabel{eq:princ_points@cref}{{[equation][4][2]2.4}{21}}
\newlabel{eq:princ_points_homog}{{2.5}{22}{2.2.2.1$\quad $RGB cameras}{equation.2.2.5}{}}
\newlabel{eq:princ_points_homog@cref}{{[equation][5][2]2.5}{22}}
\newlabel{eq:concise_form}{{2.6}{22}{2.2.2.1$\quad $RGB cameras}{equation.2.2.6}{}}
\newlabel{eq:concise_form@cref}{{[equation][6][2]2.6}{22}}
\newlabel{eq:camera_matrix}{{2.7}{22}{2.2.2.1$\quad $RGB cameras}{equation.2.2.7}{}}
\newlabel{eq:camera_matrix@cref}{{[equation][7][2]2.7}{22}}
\newlabel{eq:external_param}{{2.8}{22}{2.2.2.1$\quad $RGB cameras}{equation.2.2.8}{}}
\newlabel{eq:external_param@cref}{{[equation][8][2]2.8}{22}}
\newlabel{eq:homog_pinhole_model_camera}{{2.9}{22}{2.2.2.1$\quad $RGB cameras}{equation.2.2.9}{}}
\newlabel{eq:homog_pinhole_model_camera@cref}{{[equation][9][2]2.9}{22}}
\newlabel{eq:general_pinhole_model_camera}{{2.10}{22}{2.2.2.1$\quad $RGB cameras}{equation.2.2.10}{}}
\newlabel{eq:general_pinhole_model_camera@cref}{{[equation][10][2]2.10}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces \textbf  {CCD RGB sensor.} Picture shows an example of a very small CCD camera.\relax }}{23}{figure.caption.37}}
\newlabel{fig:ccd}{{2.16}{23}{\textbf {CCD RGB sensor.} Picture shows an example of a very small CCD camera.\relax }{figure.caption.37}{}}
\newlabel{fig:ccd@cref}{{[figure][16][2]2.16}{23}}
\@writefile{toc}{\contentsline {subsubsection}{2.2.2.2$\hskip 1em\relax $Depth Sensors and RGB-D cameras}{23}{section*.38}}
\newlabel{subsubsection:depth_rgb-d_cameras}{{2.2.2}{23}{2.2.2.2$\quad $Depth Sensors and RGB-D cameras}{section*.38}{}}
\newlabel{subsubsection:depth_rgb-d_cameras@cref}{{[subsection][2][2,2]2.2.2}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces \textbf  {Time-of-Flight sensor example.} A standard CMOS measured the distance between the object.\relax }}{23}{figure.caption.39}}
\newlabel{fig:ToF_sensor}{{2.17}{23}{\textbf {Time-of-Flight sensor example.} A standard CMOS measured the distance between the object.\relax }{figure.caption.39}{}}
\newlabel{fig:ToF_sensor@cref}{{[figure][17][2]2.17}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces \textbf  {Time-of-Flight Tera Range One sensor.} This ToF sensor is common used in many drones because of its low weight and good resolution of 640x480 pixels.\relax }}{24}{figure.caption.40}}
\newlabel{fig:terarange_one}{{2.18}{24}{\textbf {Time-of-Flight Tera Range One sensor.} This ToF sensor is common used in many drones because of its low weight and good resolution of 640x480 pixels.\relax }{figure.caption.40}{}}
\newlabel{fig:terarange_one@cref}{{[figure][18][2]2.18}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces \textbf  {SL sensor principle.} Picture shows the principle of a structured light camera. Laser triangulation scanners use either a laser line or single laser point to scan across an object. A sensor picks up the laser light that is reflected off the object, and using trigonometric triangulation, the system calculates the distance from the object to the scanner.\relax }}{25}{figure.caption.41}}
\newlabel{fig:SL_sens}{{2.19}{25}{\textbf {SL sensor principle.} Picture shows the principle of a structured light camera. Laser triangulation scanners use either a laser line or single laser point to scan across an object. A sensor picks up the laser light that is reflected off the object, and using trigonometric triangulation, the system calculates the distance from the object to the scanner.\relax }{figure.caption.41}{}}
\newlabel{fig:SL_sens@cref}{{[figure][19][2]2.19}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces \textbf  {Microsoft Kinect SL sensor.} Picture shows the structure of the Microsoft Kinect. The IR camera is a high-resolution sensor with 1280 1024 pixels, the depth-map produced by the SL depth camera is 640 x 480 pixels. The baseline between the IR camera ($C$) and the IR projector ($A$) is 75 [mm].\relax }}{26}{figure.caption.42}}
\newlabel{fig:kinect_1}{{2.20}{26}{\textbf {Microsoft Kinect SL sensor.} Picture shows the structure of the Microsoft Kinect. The IR camera is a high-resolution sensor with 1280 1024 pixels, the depth-map produced by the SL depth camera is 640 x 480 pixels. The baseline between the IR camera ($C$) and the IR projector ($A$) is 75 [mm].\relax }{figure.caption.42}{}}
\newlabel{fig:kinect_1@cref}{{[figure][20][2]2.20}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces \textbf  {An example of binary pattern projected by Microsoft Kinect.} In this representation, there is a single white pixel for each dot of the projected pattern.\relax }}{26}{figure.caption.43}}
\newlabel{fig:kinect_pattern}{{2.21}{26}{\textbf {An example of binary pattern projected by Microsoft Kinect.} In this representation, there is a single white pixel for each dot of the projected pattern.\relax }{figure.caption.43}{}}
\newlabel{fig:kinect_pattern@cref}{{[figure][21][2]2.21}{26}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Learning and Grasping}{27}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:deep_learning}{{3}{27}{Deep Learning and Grasping}{chapter.3}{}}
\newlabel{ch:deep_learning@cref}{{[chapter][3][]3}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction to Artificial Neural Networks}{27}{section.3.1}}
\newlabel{sec:ann}{{3.1}{27}{Introduction to Artificial Neural Networks}{section.3.1}{}}
\newlabel{sec:ann@cref}{{[section][1][3]3.1}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf  {Complex structure consisting of a neuron plus synapses.} On top: neuron elaborate electrical signals and pass informations to the axon; on bottom: junctions called synapses send signals to other neurons.\relax }}{28}{figure.caption.44}}
\newlabel{fig:synapse}{{3.1}{28}{\textbf {Complex structure consisting of a neuron plus synapses.} On top: neuron elaborate electrical signals and pass informations to the axon; on bottom: junctions called synapses send signals to other neurons.\relax }{figure.caption.44}{}}
\newlabel{fig:synapse@cref}{{[figure][1][3]3.1}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \textbf  {MLP Feedforward neural network structure.} On left: input layer that containing the input data; in the middle: the hidden part of the network devoted to processing the data; on the right: the output layers as the adaptation of the results obtained.\relax }}{29}{figure.caption.45}}
\newlabel{fig:ann_structure}{{3.2}{29}{\textbf {MLP Feedforward neural network structure.} On left: input layer that containing the input data; in the middle: the hidden part of the network devoted to processing the data; on the right: the output layers as the adaptation of the results obtained.\relax }{figure.caption.45}{}}
\newlabel{fig:ann_structure@cref}{{[figure][2][3]3.2}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The Perceptron and the Delta Rule}{29}{subsection.3.1.1}}
\newlabel{subsec:percdelta}{{3.1.1}{29}{The Perceptron and the Delta Rule}{subsection.3.1.1}{}}
\newlabel{subsec:percdelta@cref}{{[subsection][1][3,1]3.1.1}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \textbf  {The perceptron's structure.} Picture shows: in green, the synaptic weights refers to the strength of a connection between two nodes; in blue, a simple linear combiner as the sum of the input signals; in red, the activation function (AF) as attenuator for the amplitude of the output neuron\relax }}{30}{figure.caption.46}}
\newlabel{fig:perceptron}{{3.3}{30}{\textbf {The perceptron's structure.} Picture shows: in green, the synaptic weights refers to the strength of a connection between two nodes; in blue, a simple linear combiner as the sum of the input signals; in red, the activation function (AF) as attenuator for the amplitude of the output neuron\relax }{figure.caption.46}{}}
\newlabel{fig:perceptron@cref}{{[figure][3][3]3.3}{30}}
\newlabel{eq:AF}{{3.1}{30}{The Perceptron and the Delta Rule}{equation.3.1.1}{}}
\newlabel{eq:AF@cref}{{[equation][1][3]3.1}{30}}
\newlabel{eq:AF2}{{3.2}{30}{The Perceptron and the Delta Rule}{equation.3.1.2}{}}
\newlabel{eq:AF2@cref}{{[equation][2][3]3.2}{30}}
\newlabel{eq:AF3}{{3.3}{30}{The Perceptron and the Delta Rule}{equation.3.1.3}{}}
\newlabel{eq:AF3@cref}{{[equation][3][3]3.3}{30}}
\newlabel{eq:cell_pot}{{3.4}{31}{The Perceptron and the Delta Rule}{equation.3.1.4}{}}
\newlabel{eq:cell_pot@cref}{{[equation][4][3]3.4}{31}}
\newlabel{eq:arrays}{{3.5}{31}{The Perceptron and the Delta Rule}{equation.3.1.5}{}}
\newlabel{eq:arrays@cref}{{[equation][5][3]3.5}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \textbf  {GDR, Generalized Delta Rule.} A gradient descent learning rule applied to a MP neuron.\relax }}{31}{figure.caption.47}}
\newlabel{fig:delta_rule}{{3.4}{31}{\textbf {GDR, Generalized Delta Rule.} A gradient descent learning rule applied to a MP neuron.\relax }{figure.caption.47}{}}
\newlabel{fig:delta_rule@cref}{{[figure][4][3]3.4}{31}}
\newlabel{eq:adaption_rule}{{3.6}{31}{The Perceptron and the Delta Rule}{equation.3.1.6}{}}
\newlabel{eq:adaption_rule@cref}{{[equation][6][3]3.6}{31}}
\newlabel{eq:gradient}{{3.7}{32}{The Perceptron and the Delta Rule}{equation.3.1.7}{}}
\newlabel{eq:gradient@cref}{{[equation][7][3]3.7}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Supervised Learning}{32}{subsection.3.1.2}}
\newlabel{subsec:supervlearning}{{3.1.2}{32}{Supervised Learning}{subsection.3.1.2}{}}
\newlabel{subsec:supervlearning@cref}{{[subsection][2][3,1]3.1.2}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}The Back-propagation learning algorithm}{32}{subsection.3.1.3}}
\newlabel{subsec:backlearnalg}{{3.1.3}{32}{The Back-propagation learning algorithm}{subsection.3.1.3}{}}
\newlabel{subsec:backlearnalg@cref}{{[subsection][3][3,1]3.1.3}{32}}
\newlabel{eq:cost_funct}{{3.8}{32}{The Back-propagation learning algorithm}{equation.3.1.8}{}}
\newlabel{eq:cost_funct@cref}{{[equation][8][3]3.8}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \textbf  {$k^{th}$ neuron of the $l^{th}$ hidden layer.} A schematic representation.\relax }}{33}{figure.caption.48}}
\newlabel{fig:neuron_hidden_layer}{{3.5}{33}{\textbf {$k^{th}$ neuron of the $l^{th}$ hidden layer.} A schematic representation.\relax }{figure.caption.48}{}}
\newlabel{fig:neuron_hidden_layer@cref}{{[figure][5][3]3.5}{33}}
\newlabel{eq:deriv_AF}{{3.9}{33}{The Back-propagation learning algorithm}{equation.3.1.9}{}}
\newlabel{eq:deriv_AF@cref}{{[equation][9][3]3.9}{33}}
\newlabel{eq:linear_weights}{{3.10}{33}{The Back-propagation learning algorithm}{equation.3.1.10}{}}
\newlabel{eq:linear_weights@cref}{{[equation][10][3]3.10}{33}}
\newlabel{eq:linear_previous}{{3.11}{33}{The Back-propagation learning algorithm}{equation.3.1.11}{}}
\newlabel{eq:linear_previous@cref}{{[equation][11][3]3.11}{33}}
\newlabel{eq:deriv_chain_rule_1}{{3.12}{33}{The Back-propagation learning algorithm}{equation.3.1.12}{}}
\newlabel{eq:deriv_chain_rule_1@cref}{{[equation][12][3]3.12}{33}}
\newlabel{eq:deriv_weight}{{3.13}{34}{The Back-propagation learning algorithm}{equation.3.1.13}{}}
\newlabel{eq:deriv_weight@cref}{{[equation][13][3]3.13}{34}}
\newlabel{eq:extends_derive_calc}{{3.14}{34}{The Back-propagation learning algorithm}{equation.3.1.14}{}}
\newlabel{eq:extends_derive_calc@cref}{{[equation][14][3]3.14}{34}}
\newlabel{eq:final_extends_deriv_calc}{{3.15}{34}{The Back-propagation learning algorithm}{equation.3.1.15}{}}
\newlabel{eq:final_extends_deriv_calc@cref}{{[equation][15][3]3.15}{34}}
\newlabel{eq:local_grad}{{3.16}{34}{The Back-propagation learning algorithm}{equation.3.1.16}{}}
\newlabel{eq:local_grad@cref}{{[equation][16][3]3.16}{34}}
\newlabel{eq:local_err}{{3.17}{34}{The Back-propagation learning algorithm}{equation.3.1.17}{}}
\newlabel{eq:local_err@cref}{{[equation][17][3]3.17}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \textbf  {Back-propagation algorithm.} A complete representation of algorithm's recursion for each neuron and layer of the network.\relax }}{35}{figure.caption.49}}
\newlabel{fig:BP_alg_MLP}{{3.6}{35}{\textbf {Back-propagation algorithm.} A complete representation of algorithm's recursion for each neuron and layer of the network.\relax }{figure.caption.49}{}}
\newlabel{fig:BP_alg_MLP@cref}{{[figure][6][3]3.6}{35}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Back-Propagation Algorithm\relax }}{36}{algorithm.1}}
\newlabel{euclid}{{1}{36}{Back-Propagation Algorithm\relax }{algorithm.1}{}}
\newlabel{euclid@cref}{{[algorithm][1][]1}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Convolutional Neural Networks: advanced explanation}{37}{section.3.2}}
\newlabel{sec:cnn_advanced_explanation}{{3.2}{37}{Convolutional Neural Networks: advanced explanation}{section.3.2}{}}
\newlabel{sec:cnn_advanced_explanation@cref}{{[section][2][3]3.2}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces \textbf  {CNN layers.} A typical structure of a Convolutive Neural Network.\relax }}{37}{figure.caption.50}}
\newlabel{fig:cnn_structure}{{3.7}{37}{\textbf {CNN layers.} A typical structure of a Convolutive Neural Network.\relax }{figure.caption.50}{}}
\newlabel{fig:cnn_structure@cref}{{[figure][7][3]3.7}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces \textbf  {A convolutive filter.} The filter performs a convolution on the image as a dot products stored inside a features' map.\relax }}{37}{figure.caption.51}}
\newlabel{fig:filter}{{3.8}{37}{\textbf {A convolutive filter.} The filter performs a convolution on the image as a dot products stored inside a features' map.\relax }{figure.caption.51}{}}
\newlabel{fig:filter@cref}{{[figure][8][3]3.8}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces \textbf  {Activation maps.} The shape of an activation map is related to the number of the filters used.\relax }}{38}{figure.caption.52}}
\newlabel{fig:activ_map}{{3.9}{38}{\textbf {Activation maps.} The shape of an activation map is related to the number of the filters used.\relax }{figure.caption.52}{}}
\newlabel{fig:activ_map@cref}{{[figure][9][3]3.9}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces \textbf  {Activation function.} Pictures shows 6 of the common used activation function like.\relax }}{39}{figure.caption.53}}
\newlabel{fig:activ_func}{{3.10}{39}{\textbf {Activation function.} Pictures shows 6 of the common used activation function like.\relax }{figure.caption.53}{}}
\newlabel{fig:activ_func@cref}{{[figure][10][3]3.10}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces \textbf  {Pooling layer.} 2 kind of pooling are possible: On top, the result after performs a max pooling on the image; on bottom, the results after performs an average pooling on the image.\relax }}{39}{figure.caption.54}}
\newlabel{fig:max_avg_pool}{{3.11}{39}{\textbf {Pooling layer.} 2 kind of pooling are possible: On top, the result after performs a max pooling on the image; on bottom, the results after performs an average pooling on the image.\relax }{figure.caption.54}{}}
\newlabel{fig:max_avg_pool@cref}{{[figure][11][3]3.11}{39}}
\citation{caffe}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces \textbf  {Features inside the layers.} An example of the output detection in the point of view of the CNN.\relax }}{40}{figure.caption.55}}
\newlabel{fig:entire_layers}{{3.12}{40}{\textbf {Features inside the layers.} An example of the output detection in the point of view of the CNN.\relax }{figure.caption.55}{}}
\newlabel{fig:entire_layers@cref}{{[figure][12][3]3.12}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Frameworks involved: Caffe, Tensorflow}{40}{subsection.3.2.1}}
\newlabel{subsec:caffe_tensorflow}{{3.2.1}{40}{Frameworks involved: Caffe, Tensorflow}{subsection.3.2.1}{}}
\newlabel{subsec:caffe_tensorflow@cref}{{[subsection][1][3,2]3.2.1}{40}}
\citation{yolo_paper}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces \textbf  {CUDA.} Parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs)\relax }}{41}{figure.caption.56}}
\newlabel{fig:cuda_vga}{{3.13}{41}{\textbf {CUDA.} Parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs)\relax }{figure.caption.56}{}}
\newlabel{fig:cuda_vga@cref}{{[figure][13][3]3.13}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces \textbf  {Cuddn library.} A CUDA library of primitives for deep neural networks provided by Nvidia.\relax }}{41}{figure.caption.57}}
\newlabel{fig:cudnn_lib}{{3.14}{41}{\textbf {Cuddn library.} A CUDA library of primitives for deep neural networks provided by Nvidia.\relax }{figure.caption.57}{}}
\newlabel{fig:cudnn_lib@cref}{{[figure][14][3]3.14}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}A Noteworthy Example: YOLO Real Time Object Detection}{42}{section.3.3}}
\newlabel{sec:yolo}{{3.3}{42}{A Noteworthy Example: YOLO Real Time Object Detection}{section.3.3}{}}
\newlabel{sec:yolo@cref}{{[section][3][3]3.3}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces \textbf  {YOLO output results.} the prediction of YOLO on sample artwork and natural images from the internet.\relax }}{42}{figure.caption.58}}
\newlabel{fig:yolo_output}{{3.15}{42}{\textbf {YOLO output results.} the prediction of YOLO on sample artwork and natural images from the internet.\relax }{figure.caption.58}{}}
\newlabel{fig:yolo_output@cref}{{[figure][15][3]3.15}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces \textbf  {YOLO's architecture.}\relax }}{42}{figure.caption.59}}
\newlabel{fig:yolo_arc}{{3.16}{42}{\textbf {YOLO's architecture.}\relax }{figure.caption.59}{}}
\newlabel{fig:yolo_arc@cref}{{[figure][16][3]3.16}{42}}
\newlabel{eq:AFYOLO}{{3.20}{43}{A Noteworthy Example: YOLO Real Time Object Detection}{equation.3.3.20}{}}
\newlabel{eq:AFYOLO@cref}{{[equation][20][3]3.20}{43}}
\newlabel{eq:lossYOLO}{{3.21}{43}{A Noteworthy Example: YOLO Real Time Object Detection}{equation.3.3.21}{}}
\newlabel{eq:lossYOLO@cref}{{[equation][21][3]3.21}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces \textbf  {YOLO model.} YOLO splits the image into an 13 x 13 grid and for each grid cell predicts B bounding boxes, confidence for those boxes, and C class probabilities.\relax }}{43}{figure.caption.60}}
\newlabel{fig:yolo_model}{{3.17}{43}{\textbf {YOLO model.} YOLO splits the image into an 13 x 13 grid and for each grid cell predicts B bounding boxes, confidence for those boxes, and C class probabilities.\relax }{figure.caption.60}{}}
\newlabel{fig:yolo_model@cref}{{[figure][17][3]3.17}{43}}
\citation{yolo_paper}
\newlabel{eq:confidence_scores}{{3.22}{44}{A Noteworthy Example: YOLO Real Time Object Detection}{equation.3.3.22}{}}
\newlabel{eq:confidence_scores@cref}{{[equation][22][3]3.22}{44}}
\newlabel{eq:final_detection}{{3.23}{44}{A Noteworthy Example: YOLO Real Time Object Detection}{equation.3.3.23}{}}
\newlabel{eq:final_detection@cref}{{[equation][23][3]3.23}{44}}
\newlabel{eq:row1Yolo}{{3.24}{44}{A Noteworthy Example: YOLO Real Time Object Detection}{equation.3.3.24}{}}
\newlabel{eq:row1Yolo@cref}{{[equation][24][3]3.24}{44}}
\newlabel{eq:row2Yolo}{{3.25}{44}{A Noteworthy Example: YOLO Real Time Object Detection}{equation.3.3.25}{}}
\newlabel{eq:row2Yolo@cref}{{[equation][25][3]3.25}{44}}
\newlabel{eq:row3Yolo}{{3.26}{44}{A Noteworthy Example: YOLO Real Time Object Detection}{equation.3.3.26}{}}
\newlabel{eq:row3Yolo@cref}{{[equation][26][3]3.26}{44}}
\newlabel{eq:row4Yolo}{{3.27}{45}{A Noteworthy Example: YOLO Real Time Object Detection}{equation.3.3.27}{}}
\newlabel{eq:row4Yolo@cref}{{[equation][27][3]3.27}{45}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}The Proposed Method}{46}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:proposed_method}{{4}{46}{The Proposed Method}{chapter.4}{}}
\newlabel{ch:proposed_method@cref}{{[chapter][4][]4}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Learning s Grasp Pose from RGB-D data}{46}{section.4.1}}
\newlabel{sec:learning_rgb_depth}{{4.1}{46}{Learning s Grasp Pose from RGB-D data}{section.4.1}{}}
\newlabel{sec:learning_rgb_depth@cref}{{[section][1][4]4.1}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Regression with CNNs}{46}{section.4.2}}
\newlabel{sec:regression}{{4.2}{46}{Regression with CNNs}{section.4.2}{}}
\newlabel{sec:regression@cref}{{[section][2][4]4.2}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf  {Cells grid example.} In red, each grid of the 8x8 square cells that compose the image.\relax }}{47}{figure.caption.61}}
\newlabel{fig:grids}{{4.1}{47}{\textbf {Cells grid example.} In red, each grid of the 8x8 square cells that compose the image.\relax }{figure.caption.61}{}}
\newlabel{fig:grids@cref}{{[figure][1][4]4.1}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The used CNN}{47}{section.4.3}}
\newlabel{sec:the_used_cnn}{{4.3}{47}{The used CNN}{section.4.3}{}}
\newlabel{sec:the_used_cnn@cref}{{[section][3][4]4.3}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf  {The LMG CNN structure.}\relax }}{47}{figure.caption.62}}
\newlabel{fig:LMG_network}{{4.2}{47}{\textbf {The LMG CNN structure.}\relax }{figure.caption.62}{}}
\newlabel{fig:LMG_network@cref}{{[figure][2][4]4.2}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}The Grasping Loss Function}{48}{section.4.4}}
\newlabel{sec:the_grasping_loss_func}{{4.4}{48}{The Grasping Loss Function}{section.4.4}{}}
\newlabel{sec:the_grasping_loss_func@cref}{{[section][4][4]4.4}{48}}
\newlabel{eq:loss_grasp}{{4.1}{48}{The Grasping Loss Function}{equation.4.4.1}{}}
\newlabel{eq:loss_grasp@cref}{{[equation][1][4]4.1}{48}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Implementation details}{48}{section.4.5}}
\newlabel{sec:implementation_details}{{4.5}{48}{Implementation details}{section.4.5}{}}
\newlabel{sec:implementation_details@cref}{{[section][5][4]4.5}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf  {Image adaptation for the Network.} Picture shows the crop processing and the addition of black pixels at the nan pixels of the starting image.\relax }}{49}{figure.caption.63}}
\newlabel{fig:img_proc}{{4.3}{49}{\textbf {Image adaptation for the Network.} Picture shows the crop processing and the addition of black pixels at the nan pixels of the starting image.\relax }{figure.caption.63}{}}
\newlabel{fig:img_proc@cref}{{[figure][3][4]4.3}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces \textbf  {Depth normalization example.} In this case the normalization boundary avoid to includes all the depth points located under the board.\relax }}{49}{figure.caption.64}}
\newlabel{fig:depth_normalization}{{4.4}{49}{\textbf {Depth normalization example.} In this case the normalization boundary avoid to includes all the depth points located under the board.\relax }{figure.caption.64}{}}
\newlabel{fig:depth_normalization@cref}{{[figure][4][4]4.4}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Neighbouring Cells and Batch Generator }{49}{subsection.4.5.1}}
\newlabel{subsec:neighbouring_cells}{{4.5.1}{49}{Neighbouring Cells and Batch Generator}{subsection.4.5.1}{}}
\newlabel{subsec:neighbouring_cells@cref}{{[subsection][1][4,5]4.5.1}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces \textbf  {Near cell distance.} Picture shows the distance of the point of grasping with respect each near cell.\relax }}{50}{figure.caption.65}}
\newlabel{fig:nearest_cells}{{4.5}{50}{\textbf {Near cell distance.} Picture shows the distance of the point of grasping with respect each near cell.\relax }{figure.caption.65}{}}
\newlabel{fig:nearest_cells@cref}{{[figure][5][4]4.5}{50}}
\citation{asimov}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Custom built dataset}{51}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:custom_dataset}{{5}{51}{Custom built dataset}{chapter.5}{}}
\newlabel{ch:custom_dataset@cref}{{[chapter][5][]5}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces \textbf  {Dataset acquisition example.} The robot in an attempt to grasp objects while acquiring the dataset.\relax }}{51}{figure.caption.66}}
\newlabel{fig:acquisition_foto}{{5.1}{51}{\textbf {Dataset acquisition example.} The robot in an attempt to grasp objects while acquiring the dataset.\relax }{figure.caption.66}{}}
\newlabel{fig:acquisition_foto@cref}{{[figure][1][5]5.1}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}150 hours of try and test}{52}{section.5.1}}
\newlabel{sec:hours_try_test}{{5.1}{52}{150 hours of try and test}{section.5.1}{}}
\newlabel{sec:hours_try_test@cref}{{[section][1][5]5.1}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Dataset setup}{52}{subsection.5.1.1}}
\newlabel{subsec:dataset_setup}{{5.1.1}{52}{Dataset setup}{subsection.5.1.1}{}}
\newlabel{subsec:dataset_setup@cref}{{[subsection][1][5,1]5.1.1}{52}}
\@writefile{toc}{\contentsline {subsubsection}{5.1.1.1$\hskip 1em\relax $The robot cell}{52}{section*.67}}
\newlabel{subsubsec:the_robot_cell}{{5.1.1}{52}{5.1.1.1$\quad $The robot cell}{section*.67}{}}
\newlabel{subsubsec:the_robot_cell@cref}{{[subsection][1][5,1]5.1.1}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf  {Kinect2 external parameters calibration.} The sensor localizer calibration algorithm. The figure shows the three axes that represent the origin of the board reference system. In blue, the markers' id located by the camera.\relax }}{53}{figure.caption.68}}
\newlabel{fig:robot_cell}{{5.2}{53}{\textbf {Kinect2 external parameters calibration.} The sensor localizer calibration algorithm. The figure shows the three axes that represent the origin of the board reference system. In blue, the markers' id located by the camera.\relax }{figure.caption.68}{}}
\newlabel{fig:robot_cell@cref}{{[figure][2][5]5.2}{53}}
\@writefile{toc}{\contentsline {subsubsection}{5.1.1.2$\hskip 1em\relax $The RGB-D camera}{53}{section*.69}}
\newlabel{subsec:dataset_setup}{{5.1.1}{53}{5.1.1.2$\quad $The RGB-D camera}{section*.69}{}}
\newlabel{subsec:dataset_setup@cref}{{[subsection][1][5,1]5.1.1}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces \textbf  {The Microsoft Kinect v2 sensor.}\relax }}{53}{figure.caption.70}}
\newlabel{fig:kinect2}{{5.3}{53}{\textbf {The Microsoft Kinect v2 sensor.}\relax }{figure.caption.70}{}}
\newlabel{fig:kinect2@cref}{{[figure][3][5]5.3}{53}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces \textbf  {Technical features of Kinect v2 sensor}\relax }}{54}{table.caption.71}}
\newlabel{tab:kin_2}{{5.1}{54}{\textbf {Technical features of Kinect v2 sensor}\relax }{table.caption.71}{}}
\newlabel{tab:kin_2@cref}{{[table][1][5]5.1}{54}}
\@writefile{toc}{\contentsline {subsubsection}{5.1.1.3$\hskip 1em\relax $Calibration}{54}{section*.72}}
\newlabel{subsubsec:calibration}{{5.1.1}{54}{5.1.1.3$\quad $Calibration}{section*.72}{}}
\newlabel{subsubsec:calibration@cref}{{[subsection][1][5,1]5.1.1}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces \textbf  {Kinect2 external parameters calibration.} The sensor localizer calibration algorithm. The figure shows the three axes that represent the origin of the board reference system. In blue, the markers' id located by the camera.\relax }}{54}{figure.caption.73}}
\newlabel{fig:sens_loc}{{5.4}{54}{\textbf {Kinect2 external parameters calibration.} The sensor localizer calibration algorithm. The figure shows the three axes that represent the origin of the board reference system. In blue, the markers' id located by the camera.\relax }{figure.caption.73}{}}
\newlabel{fig:sens_loc@cref}{{[figure][4][5]5.4}{54}}
\newlabel{eq:3dWorldPoint}{{5.1}{55}{5.1.1.3$\quad $Calibration}{equation.5.1.1}{}}
\newlabel{eq:3dWorldPoint@cref}{{[equation][1][5]5.1}{55}}
\@writefile{toc}{\contentsline {subsubsection}{5.1.1.4$\hskip 1em\relax $Objects and working area}{55}{section*.74}}
\newlabel{subsubsec:objs_work_area}{{5.1.1}{55}{5.1.1.4$\quad $Objects and working area}{section*.74}{}}
\newlabel{subsubsec:objs_work_area@cref}{{[subsection][1][5,1]5.1.1}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces \textbf  {Dataset's objects.} The set of everyday objects used on the dataset's creation in real-world grasping with a robot arm.\relax }}{55}{figure.caption.75}}
\newlabel{fig:dataset_objs}{{5.5}{55}{\textbf {Dataset's objects.} The set of everyday objects used on the dataset's creation in real-world grasping with a robot arm.\relax }{figure.caption.75}{}}
\newlabel{fig:dataset_objs@cref}{{[figure][5][5]5.5}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Acquisition protocol}{55}{subsection.5.1.2}}
\newlabel{subsec:acquisition_protocol}{{5.1.2}{55}{Acquisition protocol}{subsection.5.1.2}{}}
\newlabel{subsec:acquisition_protocol@cref}{{[subsection][2][5,1]5.1.2}{55}}
\citation{flexsightThesis}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Region proposal}{56}{subsection.5.1.3}}
\newlabel{subsec:region_proposal}{{5.1.3}{56}{Region proposal}{subsection.5.1.3}{}}
\newlabel{subsec:region_proposal@cref}{{[subsection][3][5,1]5.1.3}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces \textbf  {Region Proposal example.} An example of Region Proposal output after processing the bounding boxes from YOLO detection.\relax }}{56}{figure.caption.76}}
\newlabel{fig:reg_prop}{{5.6}{56}{\textbf {Region Proposal example.} An example of Region Proposal output after processing the bounding boxes from YOLO detection.\relax }{figure.caption.76}{}}
\newlabel{fig:reg_prop@cref}{{[figure][6][5]5.6}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces \textbf  {YOLO threshold cases:} In (a), YOLO detect objects with confidence of 80$\%$; in (b), YOLO display all detection with confidence set to 0$\%$.\relax }}{57}{figure.caption.77}}
\newlabel{fig:yolo_thresh}{{5.7}{57}{\textbf {YOLO threshold cases:} In (a), YOLO detect objects with confidence of 80$\%$; in (b), YOLO display all detection with confidence set to 0$\%$.\relax }{figure.caption.77}{}}
\newlabel{fig:yolo_thresh@cref}{{[figure][7][5]5.7}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Safe depth point}{57}{subsection.5.1.4}}
\newlabel{subsec:safe_depth_point}{{5.1.4}{57}{Safe depth point}{subsection.5.1.4}{}}
\newlabel{subsec:safe_depth_point@cref}{{[subsection][4][5,1]5.1.4}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Dataset structure}{57}{subsection.5.1.5}}
\newlabel{subsec:dataset_structure}{{5.1.5}{57}{Dataset structure}{subsection.5.1.5}{}}
\newlabel{subsec:dataset_structure@cref}{{[subsection][5][5,1]5.1.5}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces \textbf  {Not registered RGB image.}\relax }}{58}{figure.caption.78}}
\newlabel{fig:noRegRgb}{{5.8}{58}{\textbf {Not registered RGB image.}\relax }{figure.caption.78}{}}
\newlabel{fig:noRegRgb@cref}{{[figure][8][5]5.8}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces \textbf  {Not registered depth image.}\relax }}{58}{figure.caption.79}}
\newlabel{fig:noRegDepth}{{5.9}{58}{\textbf {Not registered depth image.}\relax }{figure.caption.79}{}}
\newlabel{fig:noRegDepth@cref}{{[figure][9][5]5.9}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces \textbf  {Registered RGB image.}\relax }}{59}{figure.caption.80}}
\newlabel{fig:regRgb}{{5.10}{59}{\textbf {Registered RGB image.}\relax }{figure.caption.80}{}}
\newlabel{fig:regRgb@cref}{{[figure][10][5]5.10}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces \textbf  {Registered depth image.}\relax }}{59}{figure.caption.81}}
\newlabel{fig:regDepth}{{5.11}{59}{\textbf {Registered depth image.}\relax }{figure.caption.81}{}}
\newlabel{fig:regDepth@cref}{{[figure][11][5]5.11}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces \textbf  {Debug image example.} Pictures shows an example of debug image useful to understand where the robot has attempt to grasped.\relax }}{60}{figure.caption.82}}
\newlabel{fig:debugImage}{{5.12}{60}{\textbf {Debug image example.} Pictures shows an example of debug image useful to understand where the robot has attempt to grasped.\relax }{figure.caption.82}{}}
\newlabel{fig:debugImage@cref}{{[figure][12][5]5.12}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces \textbf  {Data values.} The information about the point coordinates, orientation and success or not of grasping. In (a) are reported the 3 float values relative to the x,y,z coordinates of the target pose in the base robot frame; in (b) are reported the 2 int values relative to the x,y coordinates of the registered rgb target pose in the kinect2 frame; in (c) are reported the 2 int values relative to the x,y coordinates of the not registered rgb target pose in the kinect2 frame; in (d) is reported the float value of the depth directly from the kinect2 sensor; in (e) are reported the 2 float values relative to the pitch and roll angles for the target gripper orientation; in (f) is reported the boolean value relative to the success or not of the grasping for that target pose.\relax }}{60}{figure.caption.83}}
\newlabel{fig:dataFile}{{5.13}{60}{\textbf {Data values.} The information about the point coordinates, orientation and success or not of grasping. In (a) are reported the 3 float values relative to the x,y,z coordinates of the target pose in the base robot frame; in (b) are reported the 2 int values relative to the x,y coordinates of the registered rgb target pose in the kinect2 frame; in (c) are reported the 2 int values relative to the x,y coordinates of the not registered rgb target pose in the kinect2 frame; in (d) is reported the float value of the depth directly from the kinect2 sensor; in (e) are reported the 2 float values relative to the pitch and roll angles for the target gripper orientation; in (f) is reported the boolean value relative to the success or not of the grasping for that target pose.\relax }{figure.caption.83}{}}
\newlabel{fig:dataFile@cref}{{[figure][13][5]5.13}{60}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Experiments}{61}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:experiments}{{6}{61}{Experiments}{chapter.6}{}}
\newlabel{ch:experiments@cref}{{[chapter][6][]6}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.1}CNN Setup}{61}{subsection.6.0.1}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces \textbf  {Training's parameters.} This Table shows the parameters' setup used to train the network.\relax }}{61}{table.caption.84}}
\newlabel{tab:network_training_params}{{6.1}{61}{\textbf {Training's parameters.} This Table shows the parameters' setup used to train the network.\relax }{table.caption.84}{}}
\newlabel{tab:network_training_params@cref}{{[table][1][6]6.1}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces \textbf  {Bad case of prediction's output.} Picture shows the output of a scenario in which the network has been trained in all 4 thousands images of the our dataset. In this case, there are no admissible points of grasping predicted by the network.\relax }}{62}{figure.caption.85}}
\newlabel{fig:bad_case}{{6.1}{62}{\textbf {Bad case of prediction's output.} Picture shows the output of a scenario in which the network has been trained in all 4 thousands images of the our dataset. In this case, there are no admissible points of grasping predicted by the network.\relax }{figure.caption.85}{}}
\newlabel{fig:bad_case@cref}{{[figure][1][6]6.1}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Results}{62}{section.6.1}}
\newlabel{sec:environment_setup}{{6.1}{62}{Results}{section.6.1}{}}
\newlabel{sec:environment_setup@cref}{{[section][1][6]6.1}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces \textbf  {Distribution of predicted point of grasping.} Picture shows: in red, all the predicted points with a low confidence score; in blue, the predicted point of grasping with maximum confidence score; in green, the point of grasping predicted with the second best confidence score; in yellow and orange, the 2 remaining points with an high confidence scores.\relax }}{63}{figure.caption.86}}
\newlabel{fig:distrib}{{6.2}{63}{\textbf {Distribution of predicted point of grasping.} Picture shows: in red, all the predicted points with a low confidence score; in blue, the predicted point of grasping with maximum confidence score; in green, the point of grasping predicted with the second best confidence score; in yellow and orange, the 2 remaining points with an high confidence scores.\relax }{figure.caption.86}{}}
\newlabel{fig:distrib@cref}{{[figure][2][6]6.2}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Metrics}{63}{subsection.6.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces \textbf  {Agnostic objects tested.} Picture shows the set of agnostic objects used during the experimental test.\relax }}{64}{figure.caption.87}}
\newlabel{fig:agnostic_objs}{{6.3}{64}{\textbf {Agnostic objects tested.} Picture shows the set of agnostic objects used during the experimental test.\relax }{figure.caption.87}{}}
\newlabel{fig:agnostic_objs@cref}{{[figure][3][6]6.3}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}The Maximum Confidence Score}{64}{subsection.6.1.2}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces \textbf  {The maximum confidence score results}\relax }}{64}{table.caption.88}}
\newlabel{tab:max_score_table_results}{{6.2}{64}{\textbf {The maximum confidence score results}\relax }{table.caption.88}{}}
\newlabel{tab:max_score_table_results@cref}{{[table][2][6]6.2}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}The high distribution of Confidence Score}{64}{subsection.6.1.3}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces \textbf  {The first highest confidence score results}.\relax }}{65}{table.caption.89}}
\newlabel{tab:blue_table_results}{{6.3}{65}{\textbf {The first highest confidence score results}.\relax }{table.caption.89}{}}
\newlabel{tab:blue_table_results@cref}{{[table][3][6]6.3}{65}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces \textbf  {The second highest confidence score results}\relax }}{65}{table.caption.90}}
\newlabel{tab:green_table_results}{{6.4}{65}{\textbf {The second highest confidence score results}\relax }{table.caption.90}{}}
\newlabel{tab:green_table_results@cref}{{[table][4][6]6.4}{65}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces \textbf  {The third highest confidence score results}\relax }}{65}{table.caption.91}}
\newlabel{tab:yellow_table_results}{{6.5}{65}{\textbf {The third highest confidence score results}\relax }{table.caption.91}{}}
\newlabel{tab:yellow_table_results@cref}{{[table][5][6]6.5}{65}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces \textbf  {The fourth highest confidence score results}\relax }}{65}{table.caption.92}}
\newlabel{tab:orange_table_results}{{6.6}{65}{\textbf {The fourth highest confidence score results}\relax }{table.caption.92}{}}
\newlabel{tab:orange_table_results@cref}{{[table][6][6]6.6}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces \textbf  {Bad point example 1.} Picture shows a scenario where the network fails the grasping prediction for the point with the highest confidence score (in blue). However, the network correctly predicts both second and third points with the highest confidence score (in green and in orange).\relax }}{66}{figure.caption.93}}
\newlabel{fig:example1}{{6.4}{66}{\textbf {Bad point example 1.} Picture shows a scenario where the network fails the grasping prediction for the point with the highest confidence score (in blue). However, the network correctly predicts both second and third points with the highest confidence score (in green and in orange).\relax }{figure.caption.93}{}}
\newlabel{fig:example1@cref}{{[figure][4][6]6.4}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces \textbf  {Bad point example 2.} Picture shows a scenario where the network fails the grasping prediction for the point with the highest confidence score (blue point). However, the network correctly predicts the second point with the highest confidence score (green point).\relax }}{67}{figure.caption.94}}
\newlabel{fig:example2}{{6.5}{67}{\textbf {Bad point example 2.} Picture shows a scenario where the network fails the grasping prediction for the point with the highest confidence score (blue point). However, the network correctly predicts the second point with the highest confidence score (green point).\relax }{figure.caption.94}{}}
\newlabel{fig:example2@cref}{{[figure][5][6]6.5}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Discussion}{67}{subsection.6.1.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusions}{68}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusions}{{7}{68}{Conclusions}{chapter.7}{}}
\newlabel{ch:conclusions@cref}{{[chapter][7][]7}{68}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{69}{chapter*.95}}
\bibstyle{splncs}
\bibdata{references}
\bibcite{yolo_paper}{1}
\bibcite{motion}{2}
\bibcite{cornell}{3}
\bibcite{caffe}{4}
\bibcite{hybrid_approach}{5}
\bibcite{deep_grasp}{6}
\bibcite{deep_full_image}{7}
\bibcite{epsilon_grasp}{8}
\bibcite{real_deep1}{9}
\bibcite{real_deep2}{10}
\bibcite{rectangle_7D}{11}
\bibcite{graspit}{12}
\bibcite{research_grasp}{13}
\bibcite{dart}{14}
\bibcite{deepLearningPose}{15}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{70}{chapter*.96}}
\bibcite{flexsightThesis}{16}
\bibcite{asimov}{17}
