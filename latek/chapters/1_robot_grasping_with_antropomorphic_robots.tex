\chapter{Robot Grasping with Anthropomorphic Robots}\label{ch:robotgrasping}
In this chapter we will see some applications related to grasping in robotics. Palletizing, for example, amplifies the production and sorting of large quantities of products 
within the industry itself; pick and place tasks allow the automation of the production process within the assembly lines and the safety of workers. To give another example,
increasingly up-to-date technologies allow science and chemical engineering to use pliers to grasp objects of microscopic dimensions. It is therefore clear how these applications
require high precision and coordination between two crucial components: The vision system, which typically involves the use of rgb-depth sensors, and the grasping system that includes
the robot, his controller and the gripper used. All these crucial aspects will be explained in this section as they include many of the various tools used in the robotic grasping with focus 
on two types of sensors such as the end effector grippers and cameras.

\section{Applications}\label{sec:industrial_robotics}
Industrial robotics has completely revolutionized the entire market not only in terms of production costs but also in terms of the quality of products destined for trade. The human work has literally 
been replaced by machines to the point that it is essential to have robots in all industries, so it is clear that the reliability and precision of robots play a key role in the final quality of the work,
just to mention the efficiency of a giant like Amazon that is investing much of its turnover in robotics with its section called Amazon Robotics. This section will show you some of the most used industrial
applications on robot grasping, such as the \emph{Pick and Place Task}, the \emph{Kitting Task} and the \emph{Random Bin-Picking Task}.
\subsection{Pick and Place and Kitting Applications}\label{subsec:pick_and_place_kitting_tasks}
Pick and Place Tasks (PPT) are based on grasping objects and placing them in specific locations in order to speed up the process and optimize production rates, as depicted in figure \ref{fig:pick_and_place}. Objects of
different sizes, weight and shape can be moved easily depending on the customization of the robot such as the kind of end-effector used, making the automation process simple and ductile.
In many cases of Pick and place tasks it can happen that the camera is mounted on the robotic arm itself, moreover the platform on which the arm is fixed can be mobile. A concrete example of pick and place tasks
with mobile platform is shown in figure \ref{fig:youbot_pp} and regards the Youbot used by our S.P.Q.R. team in occasion of the Robocup GermanOpen2018 competition which took place in Magdeburg.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/1_robot_grasping_with_antropomorphic_robots/pick_and_place}
    \caption{\textbf{Pick and place example.} The robot performs objects' grasping to release they in specific containers.} 
    \label{fig:pick_and_place}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/1_robot_grasping_with_antropomorphic_robots/spqr_pp}
    \caption{\textbf{Youbot Pick and place example.} The SPQR Youbot during a Robocup competition. In this task, it attempts to release the bearing to the workstation.} 
    \label{fig:youbot_pp}
\end{figure}

As far as kitting applications are concerned, the concept is to have robots that prepare the material in kits, in this way other robots can be utilize the kitted parts at full capacity. The advantage of this
approach rather than manual work is related to improve productivity, flexibility, and part flow control. Many techniques, in part forced, are adopted in order to grasp objects such as \emph{Path Recording} in 
which a human drive the robot in a desired position in order to set a specific point of grasping. This procedure makes the machine dependent on human presence, limiting its use in more complex scenarios such 
as areas with a high radioactive risk, space missions.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/1_robot_grasping_with_antropomorphic_robots/kitting}
    \caption{\textbf{Kitting task example.} A robot assembles Swiss knives in 3 containers.} 
    \label{fig:pick_and_place}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.22]{figures/1_robot_grasping_with_antropomorphic_robots/path_recording}
    \caption{\textbf{Path Recording example.} Picture shows an operator dealing with the correct positioning of the robot joints.} 
    \label{fig:pick_and_place}
\end{figure}

\subsection{The Random Bin-Picking task}\label{subsec:random_bin_task}
Random Bin-Picking task (RBP) is a process used for machine loading or separating parts from a bin for production purposes. As depicted in figure \ref{fig:rbp}, this process is commonly seen in industries dealing with
automotive stampings, molded plastics, or medical assemblies. The robot has to locate a part in free space, in an unstructured environment where the parts keep shifting positions 
and orientations every time a part is removed from the bin (figure \ref{fig:rbp_alg}). That requires a delicate balance between robotic dexterity, machine vision, software, computing power to crunch all the 
data in real time, and a grasping solution to extract the parts from the bin. Due to the chaos generated in these types of processes, accuracy in both vision and grasping for the
robot is certainly a constraint not to be overlooked, by the way the cycle time for picking and placing these parts needs typically a ranges from 5 up to 15 parts per minute.  
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/1_robot_grasping_with_antropomorphic_robots/rbp_example}
    \caption{\textbf{Random Bin-Picking example.} In this picture, a heat treat machine tending cell uses a 3D vision guided robot equipped with a dual-head end effector to locate and pick randomly
    stacked automotive parts from a large bin.} 
    \label{fig:rbp}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/1_robot_grasping_with_antropomorphic_robots/rbp_alg}
    \caption{\textbf{Execution of RBP localization algorithm with a 3D sensor vision.} In this picture, 3D area sensor maps the positions of multiple parts in a bin.} 
    \label{fig:rbp_alg}
\end{figure}
Smart gripping technology is the final challenge that robots must surmount to move random bin picking into the mainstream. The problem is that one gripper often doesn’t solve every
random resting state of a part that’s in a bin and this is the mainly reason for which the random-bin-picking is far from a solved problem.

\section{Tools}\label{sec:tools}
Since the grasping of objects requires the robot to solve two types of problems, the first related to the vision and the second to the grasping, the sensors mainly used in this
approach such as the end-effector grippers and the sensor cameras will be illustrated in detail. 
\subsection{Grippers}\label{subsec:grippers}
We humans have literally dominated planet Earth thanks to the flexibility and ease to grasp and work the objects around us. Just looking at our hands to find the answer, especially the merit is due to the thumb 
finger, the only one that is anatomically rotated of 90 degrees on the coronal axis respect to the other fingers (the figure  \ref{fig:monkey} shows how difficult it is for the rest of the animals, in this case 
a monkey, the being that closest to us, grasping a common object with their hands). Equipped only with two phalanges, i.e., proximal phalanx and distal phalanx, Thumb is able to oppose in orientation with respect to the
rest of the fingers.  
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{figures/1_robot_grasping_with_antropomorphic_robots/monkey}
    \caption{\textbf{A monkey grabbing a stick.} The opposable thumb differentiates man from the rest of the animal world, making it in fact the most evolved species in the world able to grasp and manipulate objects.} 
    \label{fig:monkey}
\end{figure}
With the advent of technology, the anatomical study of the hand has allowed us to create machines that can grab objects in the most comfortable way possible, a robot with various different end-effectors installed
therefore imply different industrial uses related to the type of object treated. The majority of grippers are variations of three fundamental designs as parallel, three-finger and angled grippers. In this section
we will see in detail most of the tools used in industrial robotics.
\subsubsection{2.2.1.1$\quad$Parallel Grippers}\label{subsubsec:parallel_grip}
This kind of tool is the most common and used gripper in the world, in particular two guides slide until they close parallel to the workpiece to grasp its outer edges. They can also work in the opposite way, opening
up to exert pressure on the interior walls. These features make this tool more accurate than other style grippers. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{figures/1_robot_grasping_with_antropomorphic_robots/parallel_gripper}
    \caption{\textbf{Schunk WSG.} A servo-electric 2 finger parallel gripper with sensitive gripping force control and long stroke.} 
    \label{fig:par_grip}
\end{figure}

\subsubsection{2.2.1.2$\quad$Parallel-Jaw Grippers}\label{subsubsection:parallel_jaw_grip}
Parallel-jaw gripper can grip both round and square parts either radially (from the side) or axially (from the top). This kind of tool is used for grasping object with a complex 
shape thanks the flexibility of his clamps that perform a sort of adaptation respect the object it self.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{figures/1_robot_grasping_with_antropomorphic_robots/parallel_jaw_gripper}
    \caption{\textbf{SPQR@Work team Robocup gripper.} A custom parallel-jaw gripper mounted in RoCoCo Lab, Sapienza.} 
    \label{fig:par_jaw_grip}
\end{figure}

\subsubsection{2.2.1.3$\quad$Three Finger Grippers}\label{subsubsection:three_finger_grip}
As depicted in Figure \ref{fig:3_fing_grip}, this tool uses three fingers. It is clear how its use is strictly limited to a few categories of graspable objects, in particular the figure \ref{fig:3_fing_grip} shows 
the different grasping modes available for this type of tool. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/1_robot_grasping_with_antropomorphic_robots/3_finger_gripper}
    \caption{\textbf{Robotiq 3FAG.} The 3-Finger Adaptive Gripper is ideal for advanced manufacturing and robotic research. It adapts to the object’s shape for a solid grip.} 
    \label{fig:3_fing_grip}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.43]{figures/1_robot_grasping_with_antropomorphic_robots/3_fing_mode}
    \caption{\textbf{3 Finger Gripper operational mode.} The basic mode is the most versatile Operation Mode. It is best suited for objects that have one dimension longer than the other two. It can grip a large
    variety of objects. The wide mode is optimal for gripping round or large objects. The pinch mode is used for small objects that have to be picked precisely. This Operation Mode can only grip objects between 
    the distal phalanxes of the fingers. The scissor mode is used primarily for tiny objects. This mode is less powerful than the other three modes, but is precise. In scissor mode, it is not possible to surround 
    an object.} 
    \label{fig:3_fing_grip}
\end{figure}

\subsubsection{2.2.1.4$\quad$Vacuum Grippers}\label{subsubsection:vacuum_gripper}
The vacuum gripper, shown in figure \ref{fig:vacuum_grip} , besides being the tool used in this thesis project, is useful for a wide variety of industries make picking, palletizing and depalletizing more efficient. 
This type of grippers provide good handling for smooth, flat, and clean objects as sheets, papers, tables and so on. It has only one surface for gripping the objects and most importantly, it is not best suitable for 
handling the objects with holes.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{figures/1_robot_grasping_with_antropomorphic_robots/vacuum_gripper}
    \caption{\textbf{Vacuum Gripper.} Vacuum gripping systems are used in a wide variety of industries to ensure efficient material flows. Pictures shows the Vacuum gripper used for
    this work of thesis.} 
    \label{fig:vacuum_grip}
\end{figure}
\subsubsection{2.2.1.5$\quad$Micro and Nano Grippers}\label{subsubsection:micro_and_nano}
Micro and Nano grippers are needed for manipulation of microscopic samples such as in the field of biology or micro assembly. Most micro manipulators rely on the principle of micro 
tweezers which grip the object, where the required force is delivered by micro actuators. In order to grasp micro objects with fragile structures and release them at their final
destination accurately, the micro-nano gripper structure must be designed carefully and efficiently, reason for which the design of this kind of gripper is a significant and hard
process. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/1_robot_grasping_with_antropomorphic_robots/micro_gripper}
    \caption{\textbf{Micro Gripper.} A micro gripper used for grasping electronic components.} 
    \label{fig:micro_grip}
\end{figure}


\subsection{Sensors}\label{subsection:sensors}
In this section we will introduce some of the most common sensors used in computer vision, fundamental devices for tasks such as PPT and RBP described above. For these sensors, acquiring the RGB channels of an 
image is a simple task, especially for CCD or CMOS cameras. However, What really interests in robotics is the 3D reconstruction of the observed points so that the depth of the image can be measured. A single
RGB camera is not enough to measure the depth information, there is a need for systems consisting of multiple RGB sensors for the reconstruction of depth. Despite this stereo vision performs a triangulation of
the image, this technique fails in the absence of essential visual features. These kind of devices are called \emph{Passive sensors} and are hardly used by modern systems, which prefer the use of a more modern
technology that includes the \emph{Active sensors}, a class of depth sensors  as well as the RGB-D sensors capable of emitting light patterns at specific wave frequencies with the target to modify, add
information to the surrounding environment facilitating the depth information measurement.

\subsubsection{2.2.2.1$\quad$RGB cameras}\label{subsubsection:rgb_cameras}
We start with the illustration of the most specialized and simplest RGB camera model: the basic pinhole camera. Progressively we will generalize this passive sensor through a series of gradations until obtain CCD sensors,
i.e., charge-coupled device, in Figure \ref{fig:ccd}. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figures/1_robot_grasping_with_antropomorphic_robots/camera_img}
    \caption{\textbf{Pinhole camera geometry.} $C$ is the camera centre and $p$ the principal point. $C$ is placed at the coordinate origin; $p$ is placed in front of the camera centre.} 
    \label{fig:camera_img}
\end{figure}
Let us consider the central projection of points in space onto a plane in which the centre of projection corresponds the origin of a Euclidean coordinate system. Consider the image 
plane $Z=f$, also called focal plane; under the pinhole camera model the 3D point $X$ is mapped to the 2D point $x$ on the image plane due to the mapping from Euclidean 3-space $\mathbb{R}^3$ to Euclidean 2-space 
$\mathbb{R}^2$ as:
\newline
\begin{equation}
(X,Y,Z)^T \to \bigg(\frac{fX}{Z},\frac{fY}{Z}\bigg)^T
\label{eq:mapping_points}
\end{equation}
\newline
The mapping \ref{eq:mapping_points} describes the central projection mapping from world to image coordinates, in little terms it represents the camera centre depicted in figure \ref{fig:camera_img}.
Figure \ref{fig:camera_img} also shows the principal axis of the camera, i.e., the line from the camera centre perpendicular to the image plane; the principal point, i.e., the point where the
principal axis meets the image plane and the principal plane of the camera, i.e., the plane through the camera centre parallel to the image plane. Using homogeneous coordinates, the central projection
can be written in matrix form as:
\newline
\begin{equation}
\begin{pmatrix}
   X \\
   Y \\
   Z \\
   1
  \end{pmatrix} \mapsto \begin{pmatrix}
   fX \\
   fY \\
   Z
  \end{pmatrix} = \begin{bmatrix}
   f &  &   & 0 \\
    & f &  & 0 \\
    &  & 1 & 0
  \end{bmatrix}
  \begin{pmatrix}
   X \\
   Y \\
   Z \\
   1
  \end{pmatrix}
  \label{eq:central_proj_homog}
 \end{equation}
 \newline
The equation \ref{eq:central_proj_homog} can be written in compactly form as:
\newline
\begin{equation}
x = PX 
\label{eq:compact_central_proj_homg}
\end{equation}
\newline
P, equation \ref{eq:compact_central_proj_homg}, represents the camera matrix for the pinhole model of central projection as the 3x4 homogeneous camera projection matrix ($diag(f,f,1)[I|0]$).
Until now, we have assumed that the principal point corresponds to the origin of coordinates in the image plane, this is a semplification of the general case, as shows in figure 
\ref{fig:principal_points}, where the coordinates of the principal point $(p_x, p_y)^T$ are unconstrained.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/1_robot_grasping_with_antropomorphic_robots/princ_point}
    \caption{\textbf{Image frame example.} Picture shows the Image $(x,y)$ and camera $(x_{cam},y_{cam})$ coordinate systems.} 
    \label{fig:principal_points}
\end{figure}

The expression \ref{eq:mapping_points} can be rewritten in general form as:
\newline
\begin{equation}
  (X Y Z)^T \mapsto \bigg(\frac{fX}{Z}+p_x,\frac{fY}{Z}+p_y\bigg)^T
 \label{eq:princ_points}
 \end{equation}
 \newline
Also in this case, the equation \ref{eq:princ_points} may be expressed in homogeneous coordinates as:
\newline
\begin{equation}
\begin{pmatrix}
   X \\
   Y \\
   Z \\
   1
  \end{pmatrix} \mapsto \begin{pmatrix}
   fX+Zp_x \\
   fY+Zp_y \\
   Z
  \end{pmatrix} = \begin{bmatrix}
   f &  & p_x & 0 \\
    & f & p_y & 0 \\
    &  & 1 & 0
  \end{bmatrix}
  \begin{pmatrix}
   X \\
   Y \\
   Z \\
   1
  \end{pmatrix}
  \label{eq:princ_points_homog}
 \end{equation}
\newline
Let us now rewrite equation \ref{eq:princ_points_homog} obtained in the following form:
\newline
 \begin{equation}
  x = K[I|0]X_{cam}
 \label{eq:concise_form}
 \end{equation}
 \newline
 we have obtained the camera calibration matrix K, also called the \emph{internal parameters} of the camera:
 \newline
 \begin{equation}
 K=
\begin{bmatrix}
   f &  & p_x \\
    & f & p_y \\
    &  & 1 
  \end{bmatrix}
  \label{eq:camera_matrix}
 \end{equation}
 \newline
To obtain the external parameters of the camera we need to extend the equation \ref{eq:concise_form} because the vector $X_{cam}$ was been assumed at the origin of a Euclidean coordinate system with the principal 
axis of the camera pointing straight down the Z-axis. Suppose $\tilde{X}$ be the inhomogeneous 3-vector coordinates of a point in the world coordinate frame and suppose also $\tilde{X_{cam}}$ be the same 
point represented in the camera coordinate frame. We can write:
\newline
 \begin{equation}
  \tilde{X}_{cam}=R(\tilde{X}-\tilde{C})
 \label{eq:external_param}
 \end{equation}
 \newline
In this expression \ref{eq:external_param} it is clear that R corresponds to a rotation  matrix, in particular to the  orientation of the camera coordinate frame; $\tilde{C}$ corresponds to the camera centre
expressed in the world coordinate frame. Also in this case, the obtained equation \ref{eq:external_param} may be expressed in homogeneous coordinates as:
\newline
\begin{equation}
  X_{cam}=
\begin{bmatrix}
  R & -R\tilde{C} \\
   0 & 1 
  \end{bmatrix} \begin{pmatrix}
   X \\
   Y \\
   Z \\
   1
  \end{pmatrix} = \begin{bmatrix}
   R & -R\tilde{C} \\
   0 & 1 
  \end{bmatrix}X
  \label{eq:homog_pinhole_model_camera}
 \end{equation}
 \newline
 At this point, we have obtained the final formula of the general mapping for a pinhole camera:
 \begin{equation}
  x = KR[I|-\tilde{C}]X
  \label{eq:general_pinhole_model_camera}
 \end{equation}
\newline
In this mapping \ref{eq:general_pinhole_model_camera}, $X$ is now in a world coordinate frame. $R$  and $\tilde{C}$ are called the \emph{external parameters} of the camera and correspond to the camera orientation
and position w.r.t. the world coordinate system. As seen before, K represents internal camera parameters. The general pinhole camera has 9 d.o.f. (3 parameters for K, R, and $\tilde{C}$) and in this case the
image coordinates have equal scales in both axial directions. In the case of CCD cameras, there is the additional possibility of 
having non-square pixels by multiplying the camera matrix K on the left by an extra factor diag($m_x$,$m_y$,1):
\begin{equation}
 K=
\begin{bmatrix}
   \alpha_x &  & x_0 \\
    & \alpha_y & y_0 \\
    &  & 1 
  \end{bmatrix}
 \end{equation}
 in which $\alpha_x$ = f$m_x$ and $\alpha_y$ = f$m_y$ represent the focal length of the camera as the pixel dimensions in the x and y direction respectively. A CCD camera has 10 dof. 
 \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.8]{figures/1_robot_grasping_with_antropomorphic_robots/ccd_camera}
    \caption{\textbf{CCD RGB sensor.} Picture shows an example of a very small CCD camera.} 
    \label{fig:ccd}
\end{figure}

\subsubsection{2.2.2.2$\quad$Depth Sensors and RGB-D cameras}\label{subsubsection:depth_rgb-d_cameras}
RGB-D cameras, also called Color-depth cameras, have become the most widely used active sensors in robotics despite the fact that many of them have been designed for video games. Color information is provided by
the RGB camera while depth information is provided, for example, by ToF cameras, i.e., \emph{time-of-flight}, laser range scanners and sensors based on SL, i.e., \emph{structured-light}. ToF sensors typically emit 
IR light at a certain wavelength, as shown in Figure \ref{fig:ToF_sensor}. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/1_robot_grasping_with_antropomorphic_robots/ToF_sensor}
    \caption{\textbf{Time-of-Flight sensor example.} A standard CMOS measured the distance between the object.} 
    \label{fig:ToF_sensor}
\end{figure}
Thanks to the use of CMOS, ToF sensors allow to obtain a full depth in real time, excellent for mobile robots especially during the SLAM phase. Obviously and unfortunately against, the quality is generally low and
accuracy is not always synonymous with warranty especially with reflective materials such as metals or black surfaces. 
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.8]{figures/1_robot_grasping_with_antropomorphic_robots/terarange}
    \caption{\textbf{Time-of-Flight Tera Range One sensor.} This ToF sensor is common used in many drones because of its low weight and good resolution of 640x480 pixels.} 
    \label{fig:terarange_one}
\end{figure}

As far as Structured Light sensors are concerned, this type of sensor is based on light triangulation. These devices encompass many system, each different due their geometry and
configuration, i.e., number of cameras, baseline, and position of the projector, and the characteristics of the projected pattern, i.e., moving pattern, moving head, dense pattern 
and so on. Hence, they are essentially a depth sensors that incorporate a laser projector inside that can project patterns of light. In this way, knowing the static distance between
the camera and the sensor projector, it is possible to measure the distance of the 3D points generated by the laser (Figure \ref{fig:SL_sens}). 
 \begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/1_robot_grasping_with_antropomorphic_robots/SL_sensor}
    \caption{\textbf{SL sensor principle.} Picture shows the principle of a structured light camera. Laser triangulation scanners use either a laser line or single laser point 
    to scan across an object. A sensor picks up the laser light that is reflected off the object, and using trigonometric triangulation, the system calculates the distance from the
    object to the scanner.} 
    \label{fig:SL_sens}
\end{figure}
A typical example of a SL sensor is the Microsoft Kinect. As shown in Figure \ref{fig:kinect_1}, the system generally comes with an RGB camera and a SL depth camera made by an IR
camera ($C$) and an IR projector ($A$).
 \begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/1_robot_grasping_with_antropomorphic_robots/kinect_v1}
    \caption{\textbf{Microsoft Kinect SL sensor.} Picture shows the structure of the Microsoft Kinect. The IR camera is a high-resolution sensor with 1280  1024 pixels, the depth-map produced
    by the SL depth camera is 640 x 480 pixels. The baseline between the IR camera ($C$) and the IR projector ($A$) is 75 [mm].} 
    \label{fig:kinect_1}
\end{figure}
As depicted in Figure \ref{fig:kinect_pattern}, this device is able to illuminate the scene with a divergent dense pattern as a pseudo-random dot matrix pattern.
 \begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/1_robot_grasping_with_antropomorphic_robots/kinect1_pattern}
    \caption{\textbf{An example of binary pattern projected by Microsoft Kinect.} In this representation, there is a single white pixel for each dot of the projected pattern.} 
    \label{fig:kinect_pattern}
\end{figure}

