\begin{abstract}
The evolution of technology opened new revolutionary scenarios in the field of robotics, making it underhand the human presence in favor of increasingly efficient and automated systems. The 
use of robots capable of grasping complicated, and heavy, objects pushes the industrial sector towards automatic palletizing, replacing human labour as far as possible. The main reasons are
purely economic, aimed to save time and production, which are essential in this field. Robotics grasping research allows machines understand which is the best way to grasp objects 
independently, and the choice of the best grasping pose. Given a CAD model, typical approaches are based on estimating the pose of the object by matching its model. Hence, the grasping pose is chosen 
according to the position and orientation in which the object has been located. We are facing with a clear problem of computer vision, a fundamental element to allow the robot to identify the region 
in the space where the gripper will perform the grasping.\\
In the last years, deep learning based systems have reached impressive results in robot grasping. Using the resources made available in the
\emph{RoCoCo} laboratory at \emph{Sapienza}, University of Rome, this thesis aims to solve the grasping problem of unknown objects by employing a self-supervised, data driven learning approach. 
% including a \emph{Microsoft Kinect2} camera rgb-depth and a \emph{ROBOX} 6 d.o.f. antropomorphic robot manipulator, 
Given an RGB image  $\mathcal{I}$ and the corresponding depth map $\mathcal{D}$ acquired by a \emph{Microsoft Kinect2} sensor, we aim to estimate the best grasping 
position for a 6 degrees of freedom (d.o.f.) anthropomorphic robot manipulator equipped with a vacuum gripper. Among other contributions, this work introduces a new set of data composed of 4000 acquisitions in which each acquisition is related to a robot that try to grasp an object. We introduce a custom Convolutional Neural Network (CNN) that has been trained with the acquired dataset.\\
  We report several experiments performed by using known (i.e., object included in the training dataset) and unknown objects, showing that our system is able to effectively learn good grasping positions.

% All this huge work aims at converging the predicted outputs by the network on the same wavelength of which the same title of this thesis work makes 
% use:  learning to grasp unknow objects with a robotic arm from experience.


 
\end{abstract} 

\chapter*{Nomenclature}\label{ch:symbols}
\addcontentsline{toc}{chapter}{Nomenclature}

%\section*{Notation}
%\refstepcounter{notation}
%\label{sec:notation}
%
%To do ...
%\begin{tabbing}
%	\hspace*{3.0cm}		\= \kill
%	$^A\rot_{B}(\alpha, \beta, \gamma)$ \> Rotation from frame A to frame B expressed in A \\[0.75ex]
%	$^A\trans_{B}(t_x,t_y,t_z)$ \> Translation from frame A to frame B expressed in A  \\[0.75ex]
%	$\T{A}{B}(R,\trans)$ \> Transformation from frame A to B expressed in A  \\[0.75ex]
%	$\t2v{A}{B}(R,\trans)$ \> Transformation from frame A to B expressed in A (vectorized) \\[0.75ex]
%	${\lfloor\trans\rfloor}_\times$ \> Skew-symmetric matrix of a vector $\mathbf{t} \in \mathbb{R}^{3\times3}$
%\end{tabbing}

\section*{Acronyms and Abbreviations}
\label{sec:acronyms}

\begin{tabbing}
	\hspace*{3.5cm}		\= \kill
	AF \> Activation Function \\[1ex]
	ANN \> Artificial Neural Network \\[1ex]
	API \> Application Programming Interface \\[1ex]
	CAFFE \> Convolutional Architecture for Fast Feature Embedding \\[1ex]
	CCD \> Charge Coupled Device \\[1ex]
	CNN \> Convolutional Neural Network \\[1ex]
	CUDA \> Compute Unified Device Architecture \\[1ex]
	GDR \> Generalized Delta Rule \\[1ex]
	GPU \> Graphical Processing Unit \\[1ex]
	IoU \> Intersection over Union \\[1ex]
	MLP NN \> Multi-layers Perceptron Neural Network \\[1ex]
	PPT \> Pick and Place Task \\[1ex]
	RBP \> Random Bin Picking \\[1ex]
	ReLU \> Rectified Linear Unit \\[1ex]
	SL \> Structured Light \\[1ex]
	ToF \> Time of Flight \\[1ex]
	YOLO \> You Only Look Once \\[1ex]
\end{tabbing}