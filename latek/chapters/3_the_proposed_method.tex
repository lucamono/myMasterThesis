\chapter{The Proposed Method}\label{ch:proposed_method}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{figures/3_the_proposed_method/mapping}
    \caption{\textbf{RGB-Depth to Grasping position mapping.} Pictures shown the mapping from an input RGB-Depth image and the 5 values that represent a point of grasping.}  
    \label{fig:mapping}
\end{figure}
As described in the introduction (chapter \ref{ch:intro}), in this thesis we aim to solve the grasping problem of unknown objects by employing a supervised, data driven learning 
approach. Differently from other similar approaches that often exploit linear gripper, we use a vacuum gripper (subsection \ref{subsec:grippers}). Similarly to \cite{real_deep1}
\cite{real_deep2}, we collect training data by means of a try-and-test procedure. We propose to employ a CNN that directly learn the grasping function from RGB-D images of the working
area. Such network is trained in an end-to-end fashion, where the grasping error is directly optimized. Similarly to \cite{yolo_paper}, we perform a regression on the entire image, in
particular, we split the image into a cells grid to achive the estimation of a grasping position for each cell of the grid. We want exploit the information obtained from the features
of the input image to predict possible grasping points for unknown objects within the image itself. Using a structure similar to that of YOLO \cite{yolo_paper}, we present a new loss
function able to minimize the predicted informations obtained from each image grid with a ground truth set of grasping points to obtain an entire distribution of grasping point
prediction. Hence, what we expect from the network output should be the prediction of admissible grasping points in areas of the image where objects are present. The network should 
also be able to learn new grasping points even on agnostic training objects. In the following sections of this chapter all these technical aspects will be explained in detail.

\section{Learning a Grasp Pose from RGB-D data}\label{sec:learning_rgb_depth}
Given an RGB image  $\mathcal{I}$ and the corresponding depth map $\mathcal{D}$, we aim to estimate the best grasping position denoted by $x,y,r,p,c$ (see Figure \ref{fig:mapping}),
where: $x,y$ represent the image coordinates of the grasping point, $r,p$ regards the gripper roll-pitch orientation relative to that point and $c$ is the confidence score. This score
is based on how confident the model is that the grasping point is located on an object and also how accurate this prediction is. In a range between 0 and 1, this score should be zero
in case of no objects located in that point or in case of wrong roll-pitch orientation, obviously, the points candidates for grasping will be the scores that are close to 1. 
We exploit the features obtained from the network's convolutive layers to performs regression on the entire image. In particular, similarly to YOLO \cite{yolo_paper}, we divide the 
image into a grid of cells (Figure \ref{fig:grids}), each one able to predict offsets from a predetermined set of grasping points. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.65]{figures/3_the_proposed_method/grids}
    \caption{\textbf{Cells grid example.} In red, the grid as the 8x8 square cells that compose the image.}  
    \label{fig:grids}
\end{figure}
We consider this predetermined set in the same way that YOLO considers anchor boxes, with the difference that we have anchor points with particular $x,y,r,p$ ratios and confidence 
score $c$ (equal to 1 in case of object grasping success, 0 in case of failure). An example of ground truth image structure is shown in Figure \ref{fig:anch_point}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{figures/3_the_proposed_method/anchor_point}
    \caption{\textbf{Ground truth anchor point example.} Picture shows an example of ground truth structure image: in red, the anchor point with the $x,y,r,p,c$ informations 
    relative to the grasp attempting; in black, all the remaining points set to 0. To be noted as the default origin of the points corresponds to the top left corner of the cell.}  
    \label{fig:anch_point}
\end{figure}
From this structure we have trained and tested our network. However, the results obtained were disappointing due to the low accuracy of the network, far below the initial
expectations. The predicted output of the network was inconsistent with the scenario because many of the position of object grasping fell on the structure
of the robot itself or in regions outside the objects and also outside the board. To overcome this problem, we will illustrate in section \ref{sec:implementation_details} a further 
change to the ground truth structure by imposing all the anchor offsets on the 3 nearest cells to the anchor points, constraining so 4 cells to the prediction of the same grasping
point. As shown in Figure \ref{fig:output_4cells}, the network has shown consistent behaviour with this new structure updated. In particular, taking into account the first 4 highest 
confidence score points predicted, we can see how the distribution of points falls within the image region containing or nearing the objects. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/3_the_proposed_method/output_4_cells}
    \caption{\textbf{Output prediction example.} Pictures shows the output prediction of the network given an input acquisition. In this case, the first 4 highest confidence score
    points predicted are located inside the objects region.}  
    \label{fig:output_4cells}
\end{figure}
\newpage
\section{CNN Architecture}\label{sec:the_used_cnn}
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{figures/3_the_proposed_method/my_custom_cnn}
    \caption{\textbf{The CNN structure.}} 
    \label{fig:LMG_network}
\end{figure}
In figure \ref{fig:LMG_network} is shown in detail the structure of the CNN used for this work of thesis. Our network has 6 convolutional layers, 3 pooling layers and 2 fully 
connected layers. For the first part of the network, 2 max-pooling are used, the last pooling layer has been choosed to perform an average of the data available, so we have
an avg-pooling before the input of the first fully connected layer. A ReLU activation function is used inside all the CNN except for the last fully connected layer in which a Sigmoid
activation function is used (section \ref{sec:cnn_advanced_explanation}). We substancially induce the network to predict a tensor of 8x8x5 with only positive values to obtain an 
implementation consistent with the ground truth batches. In this way, excluding the possibility of negative values inside the cells, the loss function will encounters all the 
predicted values from the network in the range between 0 and 1, the same range that represents the minimum and maximum distances of the points within a cell.  
We train the network for 100 epochs on the training set and we use a batch size of 16, a momentum of 0.9, a decay of 0.0 and a learning rate of $10^{âˆ’3}$. 

\section{Grasping Loss Function}\label{sec:the_grasping_loss_func}
The main and critical point of this thesis concerns the built of the loss function. Drawing inspiration from YOLO's loss function (equation \ref{eq:lossYOLO}) which needs 5 parameters \emph{x,y,w,h,c} to predict
bounding boxes, we have created the our loss function in order to predict only admissible points of grasp in the whole image. Taking into account the 5 parameters \emph{x,y,r,p,c}, coming from our dataset, i.e.,
grasping coordinates x and y, roll and pitch end-effector orientations, confidence score about success of grasp, we propose a new loss function as:
\begin{equation}
\begin{align*}
\text{loss grasping} &= \lambda_{coord} \sum_{i=0}^{S^2} \big[\big(x_i - \hat{x_i}\big)^2 + \big(y_i - \hat{y_i}\big)^2\big] \\
&+ \lambda_{orient} \sum_{i=0}^{S^2} \big[\big(r_i - \hat{r_i}\big)^2 + \big(p_i - \hat{p_i}\big)^2 \big] \\
&+ \lambda_{grasp} \sum_{i=0}^{S^2} \Big(C_i - \hat{C_i}\Big)^2 \\
&+ \lambda_{NOgrasp} \sum_{i=0}^{S^2} \Big(C_i - \hat{C_i}\Big)^2  
\end{align*}
\label{eq:loss_grasp}
\end{equation}
This loss function is able to minimize both the geometric error from the (x,y) coordinates and the algebraic error related both the roll-pitch orientation and confidence score. 
Let us see in detail the main components of Expression \ref{eq:loss_grasp}, in particular, for each grid's cell $i$ there are:
\begin{itemize}
 \item \textbf{Distance terms}: The first row shows the squared difference from two distances. The first distance is related to the predicted position of grasping $x_i$ with respect to the
 ground truth position of grasping $\hat{x_i}$. The second distance is related to predicted position of grasping $y_i$ with respect to the ground truth position of grasping $\hat{y_i}$.
 
 \item \textbf{Orientation terms}: The second row shows the squared difference from two algebraic orientations: \emph{roll} and \emph{pitch} angles. $\big(r_i - \hat{r_i}\big)^2$ concerns 
 the gripper roll orientation $r_i$ with respect to the ground truth roll orientation $\hat{r_i}$. $\big(p_i - \hat{p_i}\big)^2$ concerns the gripper pitch orientation $p_i$
 with respect to the ground truth pitch orientation $\hat{p_i}$. 
 
 \item \textbf{Confidence score}: The third and fourth lines show the squared difference from the predicted confidence score $C_i$ with respect to the ground truth confidence score 
 $\hat{C_i}$. These two formulas are managed as real mutexes according to the confidence value coming from the ground truth. In particular suppose to have $\hat{C}$ equal to 1 due
 the ground truth. It means that I had a success for an object's grasping. So only the third row of the Expression \ref{eq:loss_grasp} will contribute to the minimization of the
 error within the loss function. If instead $\hat{C}$ will assume the value 0, hence in case of object's grasping failure, only the fourth row of the Expression \ref{eq:loss_grasp}
 will contribute to the minimization of the error. This implies that in case of $\hat{C}$ equal to 1, the network must learn to predict a confidence score close to 1 in way to achive
 right behaviour. Similarly in the opposite case where $\hat{C}$ is equal to 0, the right behavior for the network must be to predict a confidence score close to 0. 
\end{itemize}
Regularisation factors have been included in our loss function. As shown in Expression \ref{eq:loss_grasp}, for distance terms and orientation terms we chose to set $\lambda_{coord}$
and $\lambda_{orient}$ equal to 1. With regard to the confidence score, to balances the ratio of positive sample numbers to negative ground truth samples we have set: $\lambda_{grasp}$
as the ratio of the number of negative examples (all that examples with $\hat{C}$ equal to 0) to the total number of examples; $\lambda_{NOgrasp}$ as the ratio of the number of
positive examples (all that examples with $\hat{C}$ equal to 1) to the total number of examples.

\section{Implementation details}\label{sec:implementation_details}
Our network works by processing 424x424x4 rgb-depth images, for this reason all the images of the dataset are readjusted from the XVAC2Network.py python file by performing a crop from 
512 x 424 pixels to 424 x 424 pixels so as to be able to manage images of square size within the network. Furthermore, additional black pixels are inserted in regions of rgb images in which the depth
information are blank due to the registered mapping between rgb and depth images. The result of this processing is shown in figure \ref{fig:img_proc}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figures/3_the_proposed_method/img_proc}
    \caption{\textbf{Image adaptation for the Network.} Picture shows the crop processing and the addition of black pixels at the nan pixels of the starting image.} 
    \label{fig:img_proc}
\end{figure}

As always happens in deep learning, once the dimensions of the images have been adjusted, they must be normalized. The python file \emph{XVAC2Numpy.py} loads, normalizes and creates two associated numpy arrays 
to feed the network. The first array contains image batches in which the rgb channels are normalized in the range from 0 to 1 and the depth channel is normalized between 0 and 1 including only the distance values 
between the board and the camera to avoid the non relevant higher distances, as depicted in figure \ref{fig:depth_normalization}. The second array contains batches of five data values \emph{x,y,r,p,c} in which
for the coordinates x,y and the roll angle a typical normalization from 0 to 1 is performed, instead for the pitch angle a custom normalization from 0 to 1 is performed due to the offset of 180 degrees for that
angle. Finally, the confidence score value can assume only 2 different values: 0 in case of not grasping and 1 in case of grasping. 
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.45]{figures/3_the_proposed_method/depth_norm}
    \caption{\textbf{Depth normalization example.} In this case the normalization boundary avoid to includes all the depth points located under the board.} 
    \label{fig:depth_normalization}
\end{figure}

\subsection{Neighbouring Cells and Batch Generator }\label{subsec:neighbouring_cells}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{figures/3_the_proposed_method/gt_grid_distance}
    \caption{\textbf{Ground truth nearest cells offset.} The ground truth grasping point includes also the distance w.r.t. the red, yellow and blue near cells.}  
    \label{fig:gt_grid_dist}
\end{figure}
As introduced previously, in YOLO network the ground truth cell contains the bounding box, i.e., the anchor bounding boxes, and the cells adjacent to the ground truth cell take into
account the distance to that specific anchor. In this way the network performs more accuracy in the prediction of the bounding box due to the fact that also the neighbouring cells
also contribute to the prediction. In our case a ground truth structure similar to this strategy adopted by the creators of YOLO  has been exploited taking into account the distance 
of the ground truth point of grasping w.r.t. the nearest cells (see Figure \ref{fig:gt_grid_dist}). The python file \emph{grids\_gt\_generator.py} performs this transformation in 
which  the entire image is splitted into 8x8 square cells of dimension 53 pixels along x and y axis (figure \ref{fig:grids}) with the reference system, for each cell, located on the
top left border of the cell. The numpy array containing the values of the grasping point for a specific image must be assigned only and exclusively to the cell of interest and to 
the other three remaining cells closer to that point, as depicted in figure \ref{fig:nearest_cells}. For all other remaining cells, the groundtruth must be set to 0. 

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.25]{figures/3_the_proposed_method/near_cells}
    \caption{\textbf{Near cell distance.} Picture shows the distance of the point of grasping with respect each near cell.} 
    \label{fig:nearest_cells}
\end{figure}
At this point we have generated all the necessary data to be able to start the training of the network.
