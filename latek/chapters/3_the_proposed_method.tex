\chapter{The Proposed Method}\label{ch:proposed_method}
As described in the introduction (Chapter \ref{ch:intro}), in this thesis we aim to solve the grasping problem of unknown objects by employing a supervised, data driven learning 
approach. Differently from other similar approaches that often involve linear gripper, we use a vacuum gripper (Subsection \ref{subsec:grippers}). Similarly to \cite{real_deep1}
\cite{real_deep2}, we collect training data by means of a try-and-test procedure. We propose to employ a CNN that directly learn the grasping function from RGB-D images of the working
area. Such network is trained in an end-to-end fashion, where the grasping error is directly optimized. Similarly to \cite{yolo_paper}, we perform a regression on the entire image, in
particular, we split the image into a cells grid to achive the estimation of a grasping position for each cell of the grid. We want to exploit the information obtained from the features
of the input image to predict possible grasping points for unknown objects within the image itself.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{figures/3_the_proposed_method/regression_model}
    \caption{\textbf{RGB-Depth to Grasping position regression.} Pictures shown the grasping problem approach from an input RGB-Depth image.}  
    \label{fig:mapping}
\end{figure}
The output of the proposed network is on the prediction of admissible grasping points in areas of the image where objects are present. The network should also be able to learn new grasping
points even on agnostic training objects. We introduce a new loss function, similar to \cite{yolo_paper}, able to minimize the predicted points of grasping in according with the ground truth
set of grasping points. In the following sections of this chapter we will provide the details of our technical choices.
\section{Learning a Grasp Pose from RGB-D data}\label{sec:learning_rgb_depth}
Given an RGB image  $\mathcal{I}$ and the corresponding depth map $\mathcal{D}$, we aim to estimate the best grasping position denoted by $x,y,\rho,\phi,C$ (see Figure \ref{fig:mapping}),
where: $x,y$ represent the image coordinates of the grasping point, $\rho,\phi$ regards the gripper roll-pitch orientation relative to that point and $C$ is the confidence score. In this work, we
assume that the depth camera provides a realiable enough depth that we use as input for grasping.

This score is based on how confident the model is that the grasping point is located on an object and also how accurate this prediction is. 
We expect that where the score is lowest, close to 0, there should be no objects. Consequently, when the score converges to 1 we expect that both the point belong to an actual object and
the orientation is suitable for a succesful grasping. 
We exploit the features obtained from the network's convolutional layers to performs regression on the entire image. In particular, similarly to YOLO \cite{yolo_paper}, we divide the 
image into a grid of cells (Figure \ref{fig:grids}), each one able to predict offsets from a predetermined set of grasping points. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.65]{figures/3_the_proposed_method/grids}
    \caption{\textbf{Cells grid example.} In red, the grid as the 8x8 square cells that compose the image.}  
    \label{fig:grids}
\end{figure}
Each cell acts as a predictor for candidate grasping points. The first implementation of our system assumed just one predictor, i.e., a grid cell, for each image. However, the results 
obtained were not convincing due to the low accuracy of the network, far below the initial expectations. Many of the position of object grasping fell on the structure of the robot itself 
or in regions outside the objects and also outside the board. To overcome this problem, we illustrate in Section \ref{sec:implementation_details} a further change to the loss function 
structure by imposing all the offsets on the 3 nearest cells to the anchor points, constraining so 4 cells to the prediction of the same grasping point. As shown in Figure \ref{fig:output_4cells}, 
the network has shown consistent behaviour with this new loss function. In particular, taking into account the first 4 highest confidence score points predicted, we can see how the predicted
points falls within the image region containing or nearing the objects. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/3_the_proposed_method/output_4_cells}
    \caption{\textbf{Output prediction example.} Pictures shows the output prediction of the network given an input acquisition. In this case, the first 4 highest confidence score
    points predicted are located inside the objects region.}  
    \label{fig:output_4cells}
\end{figure}
\newpage
\section{CNN Architecture}\label{sec:the_used_cnn}
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{figures/3_the_proposed_method/my_custom_cnn}
    \caption{\textbf{The CNN structure.}} 
    \label{fig:LMG_network}
\end{figure}
In Figure \ref{fig:LMG_network} is shown in detail the structure of the CNN used for this work of thesis. Our network has 6 convolutional layers, 3 pooling layers and 2 fully 
connected layers. For the first part of the network, 2 max-pooling are used, the last pooling layer has been choosed to perform an average of the data available, so we have
an avg-pooling before the input of the first fully connected layer. A ReLU activation function is used inside all the CNN except for the last fully connected layer in which a Sigmoid
activation function is used (Section \ref{sec:cnn_advanced_explanation}). We substancially induce the network to predict a tensor of 8x8x5 with only positive values to obtain an 
implementation consistent with the ground truth batches. In this way, excluding the possibility of negative values inside the cells, the loss function will encounters all the 
predicted values from the network in the range between 0 and 1, the same range that represents the minimum and maximum distances of the points within a cell.  
We train the network for 100 epochs on the training set and we use a batch size of 16, a momentum of 0.9, a decay of 0.0 and a learning rate of $10^{âˆ’3}$. 

\section{Grasping Loss Function}\label{sec:the_grasping_loss_func}
The main and critical point of this thesis concerns the built of the loss function. Drawing inspiration from YOLO's loss function (Equation \ref{eq:lossYOLO}) which needs 5 parameters \emph{x,y,w,h,c} to predict
bounding boxes, we have created the our loss function in order to predict only admissible points of grasp in the whole image. Taking into account the 5 parameters $x,y,\rho,\phi,C$, coming from our dataset, i.e.,
grasping coordinates x and y, roll and pitch end-effector orientations, and confidence score about success of grasp, we propose a new loss function as:
\begin{equation}
\begin{align*}
\text{loss grasping} &= \lambda_{coord} \sum_{i=0}^{S^2} \big[\big(x_i - \hat{x_i}\big)^2 + \big(y_i - \hat{y_i}\big)^2\big] \\
&+ \lambda_{orient} \sum_{i=0}^{S^2} \big[\big(\rho_i - \hat{\rho_i}\big)^2 + \big(\phi_i - \hat{\phi_i}\big)^2 \big] \\
&+ K_{grasp} \sum_{i=0}^{S^2} \Big(C_i - \hat{C_i}\Big)^2 \\
&+ K_{NOgrasp} \sum_{i=0}^{S^2} \Big(C_i - \hat{C_i}\Big)^2  
\end{align*}
\label{eq:loss_grasp}
\end{equation}
This loss function is able to minimize both the geometric error from the (x,y) coordinates and the algebraic error related to the roll-pitch orientation and confidence score. 
Let us see in detail the main components of Expression \ref{eq:loss_grasp}, in particular, for each grid's cell $i$ there are:
\begin{itemize}
 \item \textbf{Geometric error}: The first row shows the squared difference from two distances. The first distance is related to the predicted position of grasping $x_i$ with respect to the
 ground truth position of grasping $\hat{x_i}$. The second distance is related to predicted position of grasping $y_i$ with respect to the ground truth position of grasping $\hat{y_i}$.
 
 \item \textbf{Orientation error}: The second row shows the squared difference from two algebraic orientations: \emph{roll} and \emph{pitch} angles. $\big(\rho_i - \hat{\rho_i}\big)^2$ concerns 
 the gripper roll orientation $\rho_i$ with respect to the ground truth roll orientation $\hat{\rho_i}$. $\big(\phi_i - \hat{\phi_i}\big)^2$ concerns the gripper pitch orientation $\phi_i$
 with respect to the ground truth pitch orientation $\hat{\phi_i}$. 
 
 \item \textbf{Confidence score}: The third and fourth lines show the squared difference from the predicted confidence score $C_i$ with respect to the ground truth confidence score 
 $\hat{C_i}$. These two formulas are managed as real mutexes according to the confidence value coming from the ground truth. In particular suppose to have $\hat{C}$ equal to 1 due
 the ground truth. It means that I had a success for an object's grasping. So only the third row of the Expression \ref{eq:loss_grasp} will contribute to the minimization of the
 error within the loss function. If instead $\hat{C}$ will assume the value 0, hence in case of object's grasping failure, only the fourth row of the Expression \ref{eq:loss_grasp}
 will contribute to the minimization of the error. This implies that in case of $\hat{C}$ equal to 1, the network must learn to predict a confidence score close to 1 in way to achive
 right behaviour. Similarly in the opposite case where $\hat{C}$ is equal to 0, the right behavior for the network must be to predict a confidence score close to 0. 
\end{itemize}
Weights heterogeneous members have been included in our loss function. As shown in Expression \ref{eq:loss_grasp}, for geometric and orientation errors we chose to set $\lambda_{coord}$
and $\lambda_{orient}$ equal to 1. With regard to the confidence score, to balances the ratio of positive sample numbers to negative ground truth samples we have set: 
\begin{equation}
K_{grasp} = \frac{\text{negative samples}}{\text{overall samples}} = \frac{2000}{3000} = 0.667
\label{eq:regularization_grasp}
\end{equation}
\begin{equation}
K_{NOgrasp} = \frac{\text{positive samples}}{\text{overall samples}} = \frac{1000}{3000} = 0.333
\label{eq:regularization_NOgrasp}
\end{equation}
Equation \ref{eq:regularization_grasp} represents the ratio of the number of negative examples (all that examples with $\hat{C}$ equal to 0) to the total number of examples, similarly, Equation
\ref{eq:regularization_NOgrasp} represents the ratio of the number of positive examples (all that examples with $\hat{C}$ equal to 1) to the total number of examples.

\section{Implementation details}\label{sec:implementation_details}
\subsection{Preprocessing Images }\label{subsec:preproc_images}
Our network works by processing 424x424x4 rgb-depth images, for this reason all the images of the dataset are readjusted by performing a crop from 512 x 424 pixels of the original images to 424 x 424 pixels so
as to be able to manage images of square size within the network. Furthermore, additional black pixels are inserted in regions of rgb images in which the depth information are blank due to the registered mapping 
between rgb and depth images. The result of this processing is shown in Figure \ref{fig:img_proc}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figures/3_the_proposed_method/img_proc}
    \caption{\textbf{Image adaptation for the Network.} Picture shows the crop processing and the addition of black pixels at the nan pixels of the starting image.} 
    \label{fig:img_proc}
\end{figure}

As always happens in deep learning, once the dimensions of the images have been adjusted, they must be normalized. We have created an array contains image batches in which the RGB channels are normalized in the
range from 0 to 1 and the depth channel is normalized between 0 and 1. The distance values in this normalization are setting between the board and the camera to avoid the non relevant higher distances,
as depicted in figure \ref{fig:depth_normalization}. 
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.45]{figures/3_the_proposed_method/depth_norm}
    \caption{\textbf{Depth normalization example.} In this case the normalization boundary avoid to includes all the depth points located under the board.} 
    \label{fig:depth_normalization}
\end{figure}
\subsection{Training}\label{subsec:training}
The second array contains batches of five data values $x,y,\rho,\phi,C$ in which for the coordinates x,y and the roll angle a typical normalization from 0 to 1 is performed. As example values, we use a custom
normalization from 0 to 1 for $\phi$ due to the offset of 180 degrees for that angle. Finally, the confidence score value can assume only 2 different values: 0 in case of not grasping and 1 in case of grasping. 


\subsection{Neighbouring Cells and Batch Generator }\label{subsec:neighbouring_cells}
An example of ground truth image structure is shown in Figure \ref{fig:anch_point}. As introduced previously, in YOLO network the ground truth cell contains the bounding box, i.e., the anchor bounding boxes, and
the cells adjacent to the ground truth cell take into account the distance to that specific anchor. In this way the network performs more accuracy in the prediction of the bounding box due to the fact that also 
the neighbouring cells also contribute to the prediction. In our case a ground truth structure similar to this strategy adopted by the creators of YOLO  has been exploited taking into account the distance 
of the ground truth point of grasping w.r.t. the nearest cells (see Figure \ref{fig:gt_grid_dist}).
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/3_the_proposed_method/anchor_point}
    \caption{\textbf{Ground truth anchor point example.} Picture shows an example of ground truth structure image: in red, the anchor point with the $x,y,\rho,\phi,C$ informations 
    relative to the grasp attempting; in black, all the remaining points set to 0. To be noted as the default origin of the points corresponds to the top left corner of the cell.}  
    \label{fig:anch_point}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/3_the_proposed_method/gt_grid_distance}
    \caption{\textbf{Ground truth nearest cells offset.} The ground truth grasping point includes also the distance w.r.t. the red, yellow and blue near cells.}  
    \label{fig:gt_grid_dist}
\end{figure}
We perform this transformation in which  the entire image is splitted into 8x8 square cells of dimension 53 pixels along x and y axis (Figure \ref{fig:grids}) with the reference system, for each cell, located on
the top left border of the cell. The numpy array containing the values of the grasping point for a specific image must be assigned only and exclusively to the cell of interest and to the other three remaining cells 
closer to that point, as depicted in Figure \ref{fig:nearest_cells}. For all other remaining cells, the groundtruth must be set to 0. 

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.25]{figures/3_the_proposed_method/near_cells}
    \caption{\textbf{Near cell distance.} Picture shows the distance of the point of grasping with respect each near cell.} 
    \label{fig:nearest_cells}
\end{figure}