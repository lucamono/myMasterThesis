\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf {Vision and Grasping at work example.} The robot detects objects before picking them up.\relax }}{2}{figure.caption.7}
\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf {The ARMAR-III grasping.} ARMAR-III is grasping a mashed potatoes box, captured by the robot’s camera system.\relax }}{3}{figure.caption.8}
\contentsline {figure}{\numberline {1.3}{\ignorespaces \textbf {The grasping rectangle.} On left: the rgb image; on right: the registered depth map. The oriented rectangle indicates where to grasp the shoe and the gripper’s orientation. The gripper’s opening width and its physical size are represented by the red and blue lines.\relax }}{3}{figure.caption.9}
\contentsline {figure}{\numberline {1.4}{\ignorespaces \textbf {Effect of pose uncertainty on the force closure of a grasp example.} On left: planned grasp from a database of preplanned grasps with an $\epsilon _{GWS}$ of 0.17; on right: the same grasp after a 20 degrees clockwise rotation and 1 cm translation. This perturbation results in a non-force closed grasp.\relax }}{4}{figure.caption.10}
\contentsline {figure}{\numberline {1.5}{\ignorespaces \textbf {The Cornell Dataset.} On Cornell Grasping Dataset, each object has multiple labelled grasps.\relax }}{5}{figure.caption.11}
\contentsline {figure}{\numberline {1.6}{\ignorespaces \textbf {Parallel robots attempting grasp.} In this example, the acquisition of the dataset is performed for thousand hours on parallel robots until a very large number of acquisition is done.\relax }}{6}{figure.caption.12}
\contentsline {figure}{\numberline {1.7}{\ignorespaces \textbf {GraspIt! simulator.} 3D user interface allowing the user to interact with a virtual world containing robots, objects and obstacles. Each grasp is evaluated with numeric quality measures.\relax }}{6}{figure.caption.13}
\contentsline {figure}{\numberline {1.8}{\ignorespaces \textbf {DART simulator.} DART simulator includes physical forces and dynamic modelling during the grasping.\relax }}{7}{figure.caption.14}
\contentsline {figure}{\numberline {1.9}{\ignorespaces \textbf {Spqr@Work team.} Me with my team during the Robocup2018 GermanOpen competition in Magdeburg.\relax }}{8}{figure.caption.15}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces \textbf {Pick and place example.} The robot performs objects' grasping to release they in specific containers.\relax }}{10}{figure.caption.16}
\contentsline {figure}{\numberline {2.2}{\ignorespaces \textbf {Youbot Pick and place example.} The SPQR Youbot during a Robocup competition. In this task, it attempts to release the bearing to the workstation.\relax }}{11}{figure.caption.17}
\contentsline {figure}{\numberline {2.3}{\ignorespaces \textbf {Kitting task example.} A robot assembles Swiss knives in 3 containers.\relax }}{12}{figure.caption.18}
\contentsline {figure}{\numberline {2.4}{\ignorespaces \textbf {Path Recording example.} Picture shows an operator dealing with the correct positioning of the robot joints.\relax }}{12}{figure.caption.19}
\contentsline {figure}{\numberline {2.5}{\ignorespaces \textbf {Random Bin-Picking example.} In this picture, a heat treat machine tending cell uses a 3D vision guided robot equipped with a dual-head end effector to locate and pick randomly stacked automotive parts from a large bin.\relax }}{13}{figure.caption.20}
\contentsline {figure}{\numberline {2.6}{\ignorespaces \textbf {Execution of RBP localization algorithm with a 3D sensor vision.} In this picture, 3D area sensor maps the positions of multiple parts in a bin.\relax }}{14}{figure.caption.21}
\contentsline {figure}{\numberline {2.7}{\ignorespaces \textbf {A monkey grabbing a stick.} The opposable thumb differentiates man from the rest of the animal world, making it in fact the most evolved species in the world able to grasp and manipulate objects.\relax }}{15}{figure.caption.22}
\contentsline {figure}{\numberline {2.8}{\ignorespaces \textbf {Schunk WSG.} A servo-electric 2 finger parallel gripper with sensitive gripping force control and long stroke.\relax }}{16}{figure.caption.24}
\contentsline {figure}{\numberline {2.9}{\ignorespaces \textbf {SPQR@Work team Robocup gripper.} A custom parallel-jaw gripper mounted in RoCoCo Lab, Sapienza.\relax }}{16}{figure.caption.26}
\contentsline {figure}{\numberline {2.10}{\ignorespaces \textbf {Robotiq 3FAG.} The 3-Finger Adaptive Gripper is ideal for advanced manufacturing and robotic research. It adapts to the object’s shape for a solid grip.\relax }}{17}{figure.caption.28}
\contentsline {figure}{\numberline {2.11}{\ignorespaces \textbf {3 Finger Gripper operational mode.} The basic mode is the most versatile Operation Mode. It is best suited for objects that have one dimension longer than the other two. It can grip a large variety of objects. The wide mode is optimal for gripping round or large objects. The pinch mode is used for small objects that have to be picked precisely. This Operation Mode can only grip objects between the distal phalanxes of the fingers. The scissor mode is used primarily for tiny objects. This mode is less powerful than the other three modes, but is precise. In scissor mode, it is not possible to surround an object.\relax }}{18}{figure.caption.29}
\contentsline {figure}{\numberline {2.12}{\ignorespaces \textbf {Vacuum Gripper.} Vacuum gripping systems are used in a wide variety of industries to ensure efficient material flows. Pictures shows the Vacuum gripper used for this work of thesis.\relax }}{19}{figure.caption.31}
\contentsline {figure}{\numberline {2.13}{\ignorespaces \textbf {Micro Gripper.} A micro gripper used for grasping electronic components.\relax }}{19}{figure.caption.33}
\contentsline {figure}{\numberline {2.14}{\ignorespaces \textbf {Pinhole camera geometry.} $C$ is the camera centre and $p$ the principal point. $C$ is placed at the coordinate origin; $p$ is placed in front of the camera centre.\relax }}{20}{figure.caption.35}
\contentsline {figure}{\numberline {2.15}{\ignorespaces \textbf {Image frame example.} Picture shows the Image $(x,y)$ and camera $(x_{cam},y_{cam})$ coordinate systems.\relax }}{21}{figure.caption.36}
\contentsline {figure}{\numberline {2.16}{\ignorespaces \textbf {CCD RGB sensor.} Picture shows an example of a very small CCD camera.\relax }}{23}{figure.caption.37}
\contentsline {figure}{\numberline {2.17}{\ignorespaces \textbf {Time-of-Flight sensor example.} A standard CMOS measured the distance between the object.\relax }}{23}{figure.caption.39}
\contentsline {figure}{\numberline {2.18}{\ignorespaces \textbf {Time-of-Flight Tera Range One sensor.} This ToF sensor is common used in many drones because of its low weight and good resolution of 640x480 pixels.\relax }}{24}{figure.caption.40}
\contentsline {figure}{\numberline {2.19}{\ignorespaces \textbf {SL sensor principle.} Picture shows the principle of a structured light camera. Laser triangulation scanners use either a laser line or single laser point to scan across an object. A sensor picks up the laser light that is reflected off the object, and using trigonometric triangulation, the system calculates the distance from the object to the scanner.\relax }}{25}{figure.caption.41}
\contentsline {figure}{\numberline {2.20}{\ignorespaces \textbf {Microsoft Kinect SL sensor.} Picture shows the structure of the Microsoft Kinect. The IR camera is a high-resolution sensor with 1280 1024 pixels, the depth-map produced by the SL depth camera is 640 x 480 pixels. The baseline between the IR camera ($C$) and the IR projector ($A$) is 75 [mm].\relax }}{26}{figure.caption.42}
\contentsline {figure}{\numberline {2.21}{\ignorespaces \textbf {An example of binary pattern projected by Microsoft Kinect.} In this representation, there is a single white pixel for each dot of the projected pattern.\relax }}{26}{figure.caption.43}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf {Complex structure consisting of a neuron plus synapses.} On top: neuron elaborate electrical signals and pass informations to the axon; on bottom: junctions called synapses send signals to other neurons.\relax }}{28}{figure.caption.44}
\contentsline {figure}{\numberline {3.2}{\ignorespaces \textbf {MLP Feedforward neural network structure.} On left: input layer that containing the input data; in the middle: the hidden part of the network devoted to processing the data; on the right: the output layers as the adaptation of the results obtained.\relax }}{29}{figure.caption.45}
\contentsline {figure}{\numberline {3.3}{\ignorespaces \textbf {The perceptron's structure.} Picture shows: in green, the synaptic weights refers to the strength of a connection between two nodes; in blue, a simple linear combiner as the sum of the input signals; in red, the activation function (AF) as attenuator for the amplitude of the output neuron\relax }}{30}{figure.caption.46}
\contentsline {figure}{\numberline {3.4}{\ignorespaces \textbf {GDR, Generalized Delta Rule.} A gradient descent learning rule applied to a MP neuron.\relax }}{31}{figure.caption.47}
\contentsline {figure}{\numberline {3.5}{\ignorespaces \textbf {$k^{th}$ neuron of the $l^{th}$ hidden layer.} A schematic representation.\relax }}{33}{figure.caption.48}
\contentsline {figure}{\numberline {3.6}{\ignorespaces \textbf {Back-propagation algorithm.} A complete representation of algorithm's recursion for each neuron and layer of the network.\relax }}{35}{figure.caption.49}
\contentsline {figure}{\numberline {3.7}{\ignorespaces \textbf {CNN layers.} A typical structure of a Convolutive Neural Network.\relax }}{37}{figure.caption.50}
\contentsline {figure}{\numberline {3.8}{\ignorespaces \textbf {A convolutive filter.} The filter performs a convolution on the image as a dot products stored inside a features' map.\relax }}{37}{figure.caption.51}
\contentsline {figure}{\numberline {3.9}{\ignorespaces \textbf {Activation maps.} The shape of an activation map is related to the number of the filters used.\relax }}{38}{figure.caption.52}
\contentsline {figure}{\numberline {3.10}{\ignorespaces \textbf {Activation function.} Pictures shows 6 of the common used activation function like.\relax }}{39}{figure.caption.53}
\contentsline {figure}{\numberline {3.11}{\ignorespaces \textbf {Pooling layer.} 2 kind of pooling are possible: On top, the result after performs a max pooling on the image; on bottom, the results after performs an average pooling on the image.\relax }}{39}{figure.caption.54}
\contentsline {figure}{\numberline {3.12}{\ignorespaces \textbf {Features inside the layers.} An example of the output detection in the point of view of the CNN.\relax }}{40}{figure.caption.55}
\contentsline {figure}{\numberline {3.13}{\ignorespaces \textbf {CUDA.} Parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs)\relax }}{41}{figure.caption.56}
\contentsline {figure}{\numberline {3.14}{\ignorespaces \textbf {Cuddn library.} A CUDA library of primitives for deep neural networks provided by Nvidia.\relax }}{41}{figure.caption.57}
\contentsline {figure}{\numberline {3.15}{\ignorespaces \textbf {YOLO output results.} the prediction of YOLO on sample artwork and natural images from the internet.\relax }}{42}{figure.caption.58}
\contentsline {figure}{\numberline {3.16}{\ignorespaces \textbf {YOLO's architecture.}\relax }}{42}{figure.caption.59}
\contentsline {figure}{\numberline {3.17}{\ignorespaces \textbf {YOLO model.} YOLO splits the image into an 13 x 13 grid and for each grid cell predicts B bounding boxes, confidence for those boxes, and C class probabilities.\relax }}{43}{figure.caption.60}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf {Cells grid example.} In red, each grid of the 8x8 square cells that compose the image.\relax }}{47}{figure.caption.61}
\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf {The LMG CNN structure.}\relax }}{47}{figure.caption.62}
\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf {Image adaptation for the Network.} Picture shows the crop processing and the addition of black pixels at the nan pixels of the starting image.\relax }}{49}{figure.caption.63}
\contentsline {figure}{\numberline {4.4}{\ignorespaces \textbf {Depth normalization example.} In this case the normalization boundary avoid to includes all the depth points located under the board.\relax }}{49}{figure.caption.64}
\contentsline {figure}{\numberline {4.5}{\ignorespaces \textbf {Near cell distance.} Picture shows the distance of the point of grasping with respect each near cell.\relax }}{50}{figure.caption.65}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces \textbf {Dataset acquisition example.} The robot in an attempt to grasp objects while acquiring the dataset.\relax }}{51}{figure.caption.66}
\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf {Kinect2 external parameters calibration.} The sensor localizer calibration algorithm. The figure shows the three axes that represent the origin of the board reference system. In blue, the markers' id located by the camera.\relax }}{53}{figure.caption.68}
\contentsline {figure}{\numberline {5.3}{\ignorespaces \textbf {The Microsoft Kinect v2 sensor.}\relax }}{53}{figure.caption.70}
\contentsline {figure}{\numberline {5.4}{\ignorespaces \textbf {Kinect2 external parameters calibration.} The sensor localizer calibration algorithm. The figure shows the three axes that represent the origin of the board reference system. In blue, the markers' id located by the camera.\relax }}{54}{figure.caption.73}
\contentsline {figure}{\numberline {5.5}{\ignorespaces \textbf {Dataset's objects.} The set of everyday objects used on the dataset's creation in real-world grasping with a robot arm.\relax }}{55}{figure.caption.75}
\contentsline {figure}{\numberline {5.6}{\ignorespaces \textbf {Region Proposal example.} An example of Region Proposal output after processing the bounding boxes from YOLO detection.\relax }}{56}{figure.caption.76}
\contentsline {figure}{\numberline {5.7}{\ignorespaces \textbf {YOLO threshold cases:} In (a), YOLO detect objects with confidence of 80$\%$; in (b), YOLO display all detection with confidence set to 0$\%$.\relax }}{57}{figure.caption.77}
\contentsline {figure}{\numberline {5.8}{\ignorespaces \textbf {Not registered RGB image.}\relax }}{58}{figure.caption.78}
\contentsline {figure}{\numberline {5.9}{\ignorespaces \textbf {Not registered depth image.}\relax }}{58}{figure.caption.79}
\contentsline {figure}{\numberline {5.10}{\ignorespaces \textbf {Registered RGB image.}\relax }}{59}{figure.caption.80}
\contentsline {figure}{\numberline {5.11}{\ignorespaces \textbf {Registered depth image.}\relax }}{59}{figure.caption.81}
\contentsline {figure}{\numberline {5.12}{\ignorespaces \textbf {Debug image example.} Pictures shows an example of debug image useful to understand where the robot has attempt to grasped.\relax }}{60}{figure.caption.82}
\contentsline {figure}{\numberline {5.13}{\ignorespaces \textbf {Data values.} The information about the point coordinates, orientation and success or not of grasping. In (a) are reported the 3 float values relative to the x,y,z coordinates of the target pose in the base robot frame; in (b) are reported the 2 int values relative to the x,y coordinates of the registered rgb target pose in the kinect2 frame; in (c) are reported the 2 int values relative to the x,y coordinates of the not registered rgb target pose in the kinect2 frame; in (d) is reported the float value of the depth directly from the kinect2 sensor; in (e) are reported the 2 float values relative to the pitch and roll angles for the target gripper orientation; in (f) is reported the boolean value relative to the success or not of the grasping for that target pose.\relax }}{60}{figure.caption.83}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces \textbf {Bad case of prediction's output.} Picture shows the output of a scenario in which the network has been trained in all 4 thousands images of the our dataset. In this case, there are no admissible points of grasping predicted by the network.\relax }}{62}{figure.caption.85}
\contentsline {figure}{\numberline {6.2}{\ignorespaces \textbf {Distribution of predicted point of grasping.} Picture shows: in red, all the predicted points with a low confidence score; in blue, the predicted point of grasping with maximum confidence score; in green, the point of grasping predicted with the second best confidence score; in yellow and orange, the 2 remaining points with an high confidence scores.\relax }}{63}{figure.caption.86}
\contentsline {figure}{\numberline {6.3}{\ignorespaces \textbf {Agnostic objects tested.} Picture shows the set of agnostic objects used during the experimental test.\relax }}{64}{figure.caption.87}
\contentsline {figure}{\numberline {6.4}{\ignorespaces \textbf {Bad point example 1.} Picture shows a scenario where the network fails the grasping prediction for the point with the highest confidence score (in blue). However, the network correctly predicts both second and third points with the highest confidence score (in green and in orange).\relax }}{66}{figure.caption.93}
\contentsline {figure}{\numberline {6.5}{\ignorespaces \textbf {Bad point example 2.} Picture shows a scenario where the network fails the grasping prediction for the point with the highest confidence score (blue point). However, the network correctly predicts the second point with the highest confidence score (green point).\relax }}{67}{figure.caption.94}
\addvspace {10\p@ }
